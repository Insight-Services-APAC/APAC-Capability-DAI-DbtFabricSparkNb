{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Dbt Fabric Spark Notebook Generator (Dbt-FabricSparkNb)","text":"<p><p>\"The first and only dbt adapter for a true, modern, software-as-a-service (SAAS) Lakehouse.\"</p></p>"},{"location":"#what-is-it","title":"What is it","text":"<p>The <code>dbt-fabricsparknb</code> package contains all of the code enabling dbt to work with Microsoft Fabric WITHOUT a connection endpoint like livy or the Datawarehouse SQL endpoint. </p>"},{"location":"#why-did-we-build-it","title":"Why did we build it?","text":"<p>As a team of data specialists we have been working with dbt for a number of years. We have found that dbt is a powerful tool for data transformation, but it has some limitations. We have built this adapter to address some of these limitations and to make it easier to work with dbt in a modern, software-as-a-service (SAAS) lakehouse environment. </p>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>Dbt-FabricSparkNb works by leverging the power of the dbt-core, and the dbt-fabrickspark apater to create a new adapter. As such, it can be described as a \"child apater\" of dbt-fabrickspark. </p> <p>The adapter inherits all of the functionality of the dbt-fabrickspark adapter and simply extends it to meet the unique requirements of our project.</p> <p>Consequently, to use this adapter, you will need to install the dbt-fabrickspark adapter and then install the dbt-fabricksparknb adapter.</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li> Support for SAAS only lakehouse architecture (No PAAS components requried)</li> <li> Support for lightweight, disconnected local development workflow</li> <li> Fully featured with modern data transformation capabilities such as data lineage, data catalog, data quality checks and templated data transformation activities</li> <li> Opensource and free to use</li> <li> Extensible and customisable</li> </ul>"},{"location":"#limitations-and-best-practices","title":"Limitations and best practices","text":"<ul> <li>Limitation on Schema supported lakehouse: Our DBT framework does not yet support schemas in Data lake. So please do not tick \"Lakehouse schemas (Public Preview)\" when creating the lakehouse in your workspace for now.</li> </ul> <ul> <li>Unique Tables Across Lakehouses:  It is recommended to use unique table names across multiple lakehouses. This is because the current dbt framework\u2019s schema template does not support duplicate table names.</li> <li>Column Naming Conventions: Avoid using special characters such as # and % in column names within your models.</li> </ul>"},{"location":"#getting-started","title":"Getting Started","text":"<ul> <li> <p> User Guide</p> <p>For data engineers looking to use the adapter to create data transformation projects</p> <p> User Guide</p> <p>Note</p> <p>This is the guide appropriate for MOST users.</p> </li> <li> <p> Developer Guide</p> <p>For advanced users looking to extend or customise the adapter</p> <p> Developer Guide</p> <p>Danger</p> <p>This is the guide for advanced users only.</p> </li> <li> <p> Documentation Guide</p> <p>For users who are looking to contribute to the adapter documentation</p> <p> Documentation Guide</p> <p>Warning</p> <p>This guide is still under construction</p> </li> </ul>"},{"location":"#contributing","title":"Contributing","text":""},{"location":"#branching","title":"Branching","text":"<p>When creating a branch to work on from please use the branch name of <code>feature/YourBranchName</code>. The case on <code>feature/</code> matters so please make sure to keep it lower case. Pull requests are to be made into the \"dev\" branch only. Any pull requests made into \"Main\" will be removed and not merged.</p>"},{"location":"#community","title":"Community","text":""},{"location":"#logging-to-delta","title":"Logging to Delta","text":"<p>Logging was previously done to a log file saved in the lakehouse and in json format. This has been changed to now log to a delta table in the lakehouse.</p> <p>It works using 2 tables \"batch\" and \"execution_log\". At the start of the ETL the Prepare step will check if the tables exist and if they don't they will be created. This is followed by a check for an \"open\" batch and where the batch is still open it will fail. </p> <p>If you need to close the batch manually, this code is available at the end of the master notebook. </p> <p>If this check passes, a batch will be opened. There are steps in each master numbered notebook to check for failures in previousn notebook runs and this is done using the open batch so previous ETL executions with failures are not picked up and return false stops on the current execution.</p>"},{"location":"developer_guide/applications_setup/","title":"Environment Setup","text":"<p>This section outlines the steps required to setup the development environment to use this dbt-adapter as part of a dbt data transformation project.</p> <p>To provide a common, cross-platform set of instructions we will first install Powershell. To facilitate the installation process we will use package managers such as winget for Windows, brew for MacOS and <code>apt</code> for Linux.</p>"},{"location":"developer_guide/applications_setup/#core-tools-installation","title":"Core Tools Installation","text":"<p>Tip</p> <p>Following core tools can be installed using the standard powershell or in VS Code terminal (powershell)</p> WindowsMacOSLinux <pre><code># Winget Installs \nwinget install Microsoft.PowerShell\n</code></pre> <pre><code>brew install powershell/tap/powershell\n</code></pre> <pre><code># TBA\n</code></pre> <p>Next we will install Python and development tools such as vscode.</p> WindowsMacOSLinux <pre><code># Winget Installs \nwinget install Python.Python.3.12\nwinget install -e --id Microsoft.VisualStudioCode\nwinget install --id Git.Git -e --source winget\n\n# Python Environment Manager\nPython -m pip install --user virtualenv\n</code></pre> <pre><code># Brew Installs\nbrew install python@3.12\nbrew install --cask visual-studio-code\nbrew install git\n\n# Python Environment Manager\nPython -m pip install --user virtualenv\n</code></pre> <pre><code># TBA\n</code></pre>"},{"location":"developer_guide/applications_setup/#other-tools","title":"Other tools","text":"<p>Now that we have pwsh installed, Make sure that you have install the following additional required tools.</p> <ul> <li>Install Azure PowerShell on Windows - Refer Azure Powershell Doc for windows (You might need to run this in admin mode)</li> </ul> <p>Important</p> <p>Optional packages you may need to install (Only run if you face issues)</p> <pre><code>pip config set global.trusted-host \"pypi.org files.pythonhosted.org pypi.python.org\"\n</code></pre>"},{"location":"developer_guide/applications_setup/#source-directory-python-env","title":"Source Directory &amp; Python Env","text":"<p>Now lets create and activate our Python environment and install the required packages. Be sure to run the following commands from the root directory.</p> <p>Tip</p> <p>When executing the following, it can take a few minutes to complete on some machines. Occasionally pip may get stuck and in such cases break the execution using ctrl-c and run the same pip again. </p> WindowsMacOSLinux <pre><code># Ensure that you are in the pwsh shell\npwsh\n\n# Create a new source code directory\nmkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like\n# Navigate to the new directory\ncd dbt-fabricsparknb-test\n\n# Create the Python environment\npython -m venv .env\n\n#Optional step to run if activate.ps1 failes due to security policy\nSet-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope CurrentUser\n\n# Activate the Python environment\n./.env/Scripts/Activate.ps1\n</code></pre> <pre><code># Ensure that you are in the pwsh shell\npwsh\n\n# Create a new source code directory\nmkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like\n# Navigate to the new directory\ncd dbt-fabricsparknb-test\n\n# Create the Python environment\npython -m venv .env\n\n#Optional step to run if activate.ps1 failes due to security policy\nSet-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope CurrentUser\n\n# Activate the Python environment\n./.env/Scripts/Activate.ps1  \n</code></pre> <pre><code># TBA\n</code></pre> <p>This concludes the required applications.</p> <p>Info</p> <p>You are now ready to move to the next step in which you will set up your dbt project. Follow the Dbt Project Setup guide.</p>"},{"location":"developer_guide/dbt_build_process/","title":"Dbt Build Process","text":""},{"location":"developer_guide/dbt_build_process/#dbt-build-process-dbt_wrapper","title":"Dbt Build Process &amp; Dbt_Wrapper","text":"<p>The dbt-fabricksparknb package includes a console application that will allow you to build your dbt project and generate a series of notebooks that can be run in a Fabric workspace. This application is called <code>dbt_wrapper</code> and is a python script that is run from the command line. You can invoke the application and view information about it by running the following command in a terminal.</p> <p>Important</p> <p>Before running the dbt_wrapper make sure you're logged into your tenant in the PowerShell terminal using both az login. See the examples below and replace the tenant id with your own.</p> <pre><code>az login --tenant 73738727-cfc1-4875-90c2-2a7a1149ed3d --allow-no-subscriptions\n</code></pre> <p>If you're encountering an az not found error, install az using the following command (powershell admin mode) <pre><code>Install-Module -Name Az -Repository PSGallery -Force\n</code></pre></p> <p>Note</p> <p>Make sure that you have activated your python virtual environment before running this code. </p> <pre><code>python test_pre_install.py run-all --help\n</code></pre> <p>To build your dbt project and publish your notebook to your Fabric workspace you can run the command below:</p> <p>Note</p> <p>Be sure to replace my_project with the name of your dbt project folder. </p> <pre><code>python test_pre_install.py run-all --pre-install my_project \n</code></pre> <p>The command above will carry out all of the necessary \"stages\" required to fully build your dbt project and generate the notebooks that can be run in a Fabric workspace. When run successfully your should see output similar to the image below. </p>"},{"location":"developer_guide/dbt_build_process/#toggling-build-stages-off-and-on","title":"Toggling Build Stages Off and On","text":"<p>There are times when you may not wish to run ALL of the build steps. In such circumstances you can toggle off specific stages by using the options built in to the dbt_wrapper application. To view all of the options available to you run the command below: <pre><code>python test_pre_install.py run-all --help\n</code></pre></p> <p>For example, should you wish to run all stages except for the upload of the generated notebooks to your Fabric workspace you can run the command below: <pre><code>python test_pre_install.py run-all my_project --no-upload-notebooks-via-api  \n</code></pre></p> <p>Alternatively, you might want to make use of some additional \"helper\" commands that we have included in the application.  For example, Notebooks are defaulted to the timeout of 1800 seconds. You can increase that by passing a timeout configuration when building the project. </p> <p>Note</p> <p>you can change the int value to anything you want as long as it's not larger than 7 days in seconds</p> <pre><code>python test_pre_install.py run-all --pre-install --notebook-timeout=2100 my_project\n</code></pre> <p>Review all of the commands available to you by running using the help option as shown below: <pre><code>python test_pre_install.py --help\n</code></pre></p> <p>Info</p> <p>You are now ready to move to the next step in which you gain an understanding of the various kinds of notebooks generated by the adapter. Follow the Understanding the Generated Notebooks guide.</p>"},{"location":"developer_guide/dbt_setup/","title":"Setting Up dbt","text":"<p>The following sections are covered in this document:</p> <ol> <li>Clone the main Repo<ol> <li>Clone the main repo in powershell</li> <li>Clone the repo in VSCode</li> </ol> </li> <li>dbt dependency installation</li> </ol>"},{"location":"developer_guide/dbt_setup/#a-clone-the-main-repo-in-powershell","title":"a. Clone the main repo in powershell","text":"<p>Clone the main repo from the Github. Use the following pwsh code for that. </p> <pre><code># Clone the repo\ngit clone https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb.git\n</code></pre>"},{"location":"developer_guide/dbt_setup/#b-clone-the-main-repo-in-vscode","title":"b. Clone the main repo in VSCode","text":"<p>Clone the main repo locally using Visual studio code. </p> <p>Note</p> <p>If you do not get the Clone Repository option when selecting Source Control from the menu, then you have not installed GIT and will need to complete that first.</p> <p></p>"},{"location":"developer_guide/dbt_setup/#dbt-dependency-installation","title":"dbt dependency installation","text":"<p>Info</p> <p>Ensure you remain in the PowerShell console with your virtual environment activated.  You can then execute the following command to install or update all the components needed for the dbt framework. Since the <code>requirements.txt</code> file is located in the root of the repository, be sure to run the command from the root directory.</p> <pre><code># dbt dependency installation/updation\npip install -r requirements.txt\n</code></pre> <p>Note</p> <p>The installation or update process might take some time to complete and could appear to be hanging, but it is actively running. If you need to stop the process, you can restart it using the same command. It will bypass any components that are already installed.</p> <p>This concludes the dbt installation.</p> <p>Info</p> <p>You are now ready to move to the next step in which you set up the build process. Follow the Dbt Build Process guide.</p>"},{"location":"developer_guide/framework_setup/","title":"Framework Setup","text":""},{"location":"developer_guide/framework_setup/#fabric-workspace-setup","title":"Fabric Workspace Setup","text":"<p>Here we will create a new dbt project and configure it to use the dbt-fabricsparknb adapter. But, before we do this we need to gather some information from the Power BI / Fabric Portal. To do this, follow the steps below:</p> <ol> <li>Open the Power BI Portal and navigate to the workspace you want to use for development. If necessary, create a new workspace.</li> <li>Ensure that the workspace is Fabric enabled. If not, enable it.</li> <li>Make sure that there is at least one Datalake in the workspace.</li> <li>Get the connection details for the workspace. <ol> <li>Get the lakehouse name, the workspace id, and the lakehouse id. </li> <li>The lakehouse name and workspace name are easily viewed from the fabric / power bi portal.</li> <li>The easiest way to get this information is to <ol> <li>Navigate to a file or folder in the lakehouse, </li> <li>click on the three dots to the right of the file or folder name, and select \"Properties\". Details will be displayed in the properties window. </li> <li>From these properties select copy url and paste it into a text editor. The workspace id is the first GUID in the URL, the lakehouse id is the second GUID in the URL. </li> <li>In the example below, the workspace id is <code>4f0cb887-047a-48a1-98c3-ebdb38c784c2</code> and the lakehouse id is <code>aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9</code>.</li> </ol> </li> </ol> </li> </ol> <p>https://onelake.dfs.fabric.microsoft.com/4f0cb887-047a-48a1-98c3-ebdb38c784c2/aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9/Files/notebooks</p>"},{"location":"developer_guide/framework_setup/#create-dbt-project","title":"Create Dbt Project","text":"<p>Once you have taken note of the workspace id, lakehouse id, and lakehouse name, you can create a new dbt project and configure it to use the dbt-fabricsparknb adapter. To do this, run the code shown below:</p> <p>!&gt; Important Note when asked to select the adapter choose <code>dbt-fabricksparknb</code>. If you can't see the adapter, first install the dbt-fabricsparknb package from repository. During this process you will also be asked for the <code>workspace id</code>, <code>lakehouse id</code>, and <code>lakehouse name</code>. Use the values you gathered from the Power BI Portal. </p> <pre><code># Create your dbt project directories and profiles.yml file\ndbt init my_project # Note that the name of the project is arbitrary... call it whatever you like\n</code></pre> <p>The command above will create a new directory called <code>my_project</code>. Within this directory you will find a <code>dbt_project.yml</code> file. Open this file in your favourite text editor and note that it should look like the example below except that in your case my_project will be replaced with the name of the project you created above.:</p> <pre><code># Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'my_project'\nversion: '1.0.0'\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'my_project'\n\n# These configurations specify where dbt should look for different types of files.\n# The `model-paths` config, for example, states that models in this project can be\n# found in the \"models/\" directory. You probably won't need to change these!\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\nclean-targets:         # directories to be removed by `dbt clean`\n  - \"target\"\n  - \"dbt_packages\"\n\n\n# Configuring models\n# Full documentation: https://docs.getdbt.com/docs/configuring-models\n\n# In this example config, we tell dbt to build all models in the example/\n# directory as views. These settings can be overridden in the individual model\n# files using the ` config(...) ` macro.\nmodels:\n  test4:\n    # Config indicated by + and applies to all files under models/example/\n    example:\n      +materialized: view\n</code></pre> <p>The dbt init command will also update your <code>profiles.yml</code> file with a profile matching your dbt project name.  Open this file in your favourite text editor using the command below:</p> WindowsMacOSLinux <pre><code>code  $home/.dbt/profiles.yml\n</code></pre> <pre><code>code  ~/.dbt/profiles.yml\n</code></pre> <pre><code>code  ~/.dbt/profiles.yml\n</code></pre> <p>When run this will display a file similar to the one below. Check that your details are correct. </p> <pre><code>my_project:\n  outputs:\n    dev:\n      auth: cli #remove\n      client_id: dlkdjl #remove\n      client_scrent: dlkdjl #remove\n      connect_retries: 0 #remove\n      connect_timeout: 0 #remove\n      endpoint: dkld #remove\n      lakehouse: 'lakehouse' #the name of your lakehouse\n      lakehouseid: 'aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9' #the guid of your lakehouse\n      method: livy\n      schema: dbo #the schema you want to use\n      tenant_id: '72f988bf-86f1-41af-91ab-2d7cd011db47' #your power bi tenant id\n      threads: 1 #the number of threads to use\n      type: fabricsparknb #the type of adapter to use.. always use fabricsparknb\n      workspaceid: '4f0cb887-047a-48a1-98c3-ebdb38c784c2' #the guid of your workspace\n  target: dev\n</code></pre> <p>This concludes the Framework setup.</p> <p>Info</p> <p>You are now ready to move to the next step in which you will set up your dbt project. Follow the dbt Build Process guide.</p>"},{"location":"developer_guide/generated_notebooks/","title":"Understanding the Generated Notebooks","text":""},{"location":"developer_guide/generated_notebooks/#understanding-the-notebooks-generated","title":"Understanding the Notebooks Generated","text":"<p>After the successful execution of the build script, you will see a series of notebooks generated in your my_project/target/notebooks directory. This is the <code>special sauce</code> of this dbt-adapter that allows your to run your dbt project natively as notebooks in a Fabric workspace. The image below shows a sample listing of generated notebooks. Your specific notebooks will be contain the name of your dbt project and may be different depending on the models and tests that you have defined in your dbt project.</p> <p>Sample listing of Generated Notebooks</p> <p></p> <p>If you study the files shown above you will notice that there is a naming convention and that the notebooks are prefixed with a specific string. The following table explains at a high level the naming convention and the purpose of each type of notebook.</p> Notebook Prefix Description model. These are dbt model notebooks. A notebook will be generated for each dbt model that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. test. These are dbt test notebooks. A notebook will be generated for each dbt test that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. seed. These are dbt seed notebooks. A notebook will be generated for each dbt seed that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. master_ These are execution orchestration notebooks. They allow the running of your models, tests and seeds in parallel and in the correct order. They are what allow you to run your transformation pipelines independently of dbt as an orchestrator. In order to run your project simply schedule master.{project_name}.notebook.iypnb using Fabric's native scheduling functionality import_ This is a helper notebook that facilitate import of generated notebooks into workspace. metadata_ This is a helper notebook to facilitate generation of workspace metadata json files. <p>Important</p> <p>The green panels below provide a more detailed discussion of each type of notebook. Take a moment to expand each panel by clicking on it and read the detailed explanation of each type of notebook.</p> Notebooks with the Prefix <code>\"model.\"</code> <p>These are dbt model notebooks. A notebook will be generated for each dbt model that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt.</p> <p></p> <p></p> Notebooks with the Prefix <code>\"test.\"</code> <p>These are dbt test notebooks. A notebook will be generated for each dbt test that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt.</p> Notebooks with the Prefix <code>\"seed.\"</code> <p>These are dbt seed notebooks. A notebook will be generated for each dbt seed that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt.</p> Notebooks with the Prefix <code>\"master_\"</code> <p>These are execution orchestration notebooks. They allow the running of your models, tests and seeds in parallel and in the correct order. They are what allow you to run your transformation pipelines independently of dbt as an orchestrator. In order to run your project simply schedule master.{project_name}.notebook.iypnb using Fabric's native scheduling functionality.</p> Notebooks with the Prefix <code>\"import_\"</code> <p>This is a helper notebook that facilitates import of generated notebooks into workspace.</p> Notebooks with the Prefix <code>\"metadata_\"</code> <p>This is a helper notebook to facilitates the generation of workspace metadata json files.</p>"},{"location":"developer_guide/generated_notebooks/#notebooks-in-your-fabric-workspace","title":"Notebooks in your Fabric Workspace","text":"<p>If you login to your fabric workspace and navigate to the notebooks section you will see that the generated notebooks have been uploaded to your workspace.</p> <p>Tip</p> <p>I suggest that you move your notebooks into a folder that matches the name of your dbt project.</p>"},{"location":"documentation_guide/","title":"Documentation Guide","text":""},{"location":"documentation_guide/#building-you-environment","title":"Building you environment","text":"<p>Documentation for this project is built using mkdocs-material. To contribute to the documentation you will need to create a separate python environment. I suggest that you call this <code>.env_mkdocs</code> to avoid confusion with the dbt environment. </p> <p>Important</p> <p>The commands below assume that you have already performed the <code>Core Tools Installation</code> steps in the User Guide. If you have not done this yet, please do so before proceeding. Note you ONLY have to install <code>core tools</code> it is not necessary to move on to the <code>other tools</code> section. </p> <p>Before creating the environment you will need to clone the repository. You can do this by running the command below:</p> <p>clone the repository<pre><code>git clone https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb.git MyDocsProject\n</code></pre> This will clone the repository into a directory called MyDocsProject. You can rename this directory to whatever you like. Navigate into this new directory and then run the commands below.</p> Create and activate the Python environment<pre><code># Create the Python environment\npython -m venv .env_mkdocs\n\n# Activate the Python environment\n.\\.env_mkdocs\\Scripts\\activate.ps1\n\n#Install the mkdocs packages\npip install -r ./requirements_mkdocs.txt\n</code></pre> <p>These commands will create a new python environment and install the required packages for building the documentation. </p>"},{"location":"documentation_guide/#updating-the-documentation","title":"Updating the documentation","text":"<p>The documentation source is held in the <code>docs</code> directory. To update the documentation you will need to edit the markdown files in this directory. In order to understand the syntax used for the markdown be sure to review the reference section for mkdocs-material. Once you have made your changes you can build the documentation using the command below:</p> Build the documentation<pre><code>mkdocs build\n</code></pre> <p>To view the documentation locally you can use the command below:</p> View the documentation locally<pre><code>mkdocs serve\n</code></pre> <p>Tip</p> <p>The <code>mkdocs serve</code> command will start a local web server that will allow you to view the documentation in your browser. The server will also automatically rebuild the documentation when you make changes to the source files.</p> <p>Before publishing the documentation you should ensure that the documentation is up to date and that the changes are correct. You should also pull the latest from the repository to ensure that you are not overwriting someone else's changes. Do this by running the command below:</p> Pull the latest changes from the repository<pre><code>git pull\n</code></pre> <p>You can now publish the documentation to the repository by running the command below:</p> Publish the documentation<pre><code>mkdocs gh-deploy\n</code></pre>"},{"location":"user_guide/","title":"User Guide","text":""},{"location":"user_guide/dbt_build_process/","title":"Dbt Build Process","text":""},{"location":"user_guide/dbt_build_process/#dbt-build-process-dbt_wrapper","title":"Dbt Build Process &amp; Dbt_Wrapper","text":"<p>The dbt-fabricksparknb package includes a console application that will allow you to build your dbt project and generate a series of notebooks that can be run in a Fabric workspace. This application is called <code>dbt_wrapper</code> and is a python script that is run from the command line. You can invoke the application and view information about it by running the following command in a terminal.</p> <p>Important</p> <p>Before running the dbt_wrapper make sure you're logged into your tenant in the PowerShell terminal using both az login. See the examples below and replace the tenant id with your own.</p> <pre><code>az login --tenant 73738727-cfc1-4875-90c2-2a7a1149ed3d --allow-no-subscriptions\n</code></pre> <p>Note</p> <p>Make sure that you have activated your python virtual environment before running this code. </p> <pre><code>dbt_wrapper --help\n</code></pre> <p>To build your dbt project and publish your notebook to your Fabric workspace you can run the command below:</p> <p>Note</p> <p>Be sure to replace my_project with the name of your dbt project folder</p> <pre><code>dbt_wrapper run-all my_project\n</code></pre> <p>The command above will carry out all of the necessary \"stages\" required to fully build your dbt project and generate the notebooks that can be run in a Fabric workspace. When run successfully your should see output similar to the image below.</p> <p></p>"},{"location":"user_guide/dbt_build_process/#toggling-build-stages-off-and-on","title":"Toggling Build Stages Off and On","text":"<p>There are times when you may not wish to run ALL of the build steps. In such circumstances you can toggle off specific stages by using the options built in to the <code>dbt_wrapper</code> application. To view all of the options available to you run the command below:</p> <pre><code>dbt_wrapper run-all --help\n</code></pre> <p>For example, should you wish to run all stages except for the upload of the generated notebooks to your Fabric workspace you can run the command below:</p> <p><pre><code>dbt_wrapper run-all my_project --no-upload-notebooks-via-api  \n</code></pre> Alternatively, you might want to make use of some additional \"helper\" commands that we have included in the application. For example, Notebooks are defaulted to the timeout of 1800 seconds. You can increase that by passing a timeout configuration when building the project. </p> <p>Note</p> <p>you can change the int value to anything you want as long as it's not larger than 7 days in seconds</p> <pre><code>dbt_wrapper run-all my_project --notebook-timeout=2100\n</code></pre> <p>Review all of the commands available to you by running using the help option as shown below:</p> <pre><code>dbt_wrapper --help\n</code></pre> <p>Info</p> <p>You are now ready to move to the next step in which you gain an understanding of the various kinds of notebooks generated by the adapter. Follow the Understanding the Generated Notebooks guide.</p>"},{"location":"user_guide/dbt_project_setup/","title":"DBT Project Setup","text":""},{"location":"user_guide/dbt_project_setup/#fabric-workspace-setup","title":"Fabric Workspace Setup","text":"<p>Next we will create a new dbt project and configure it to use the dbt-fabricsparknb adapter. But, before we do this we need to gather some information from the Power BI / Fabric Portal. To do this, follow the steps below:</p> <ol> <li>Open the Power BI Portal and navigate to the workspace you want to use for development. If necessary, create a new workspace.</li> <li>Ensure that the workspace is Fabric enabled. If not, enable it.</li> <li>Make sure that there is at least one Datalake in the workspace.</li> <li> <p>Get the connection details for the workspace.</p> <ol> <li>You will need to get the workspace name, workspace id, lakehouse id, and lakehouse name. </li> <li>The lakehouse name and workspace name are easily viewed from the fabric / power bi portal. </li> <li>The easiest way to get the id information is to:<ol> <li>Navigate to a file or folder in your target lakehouse.</li> <li>Click on the three dots to the right of the file or folder name, and select \"Properties\". Details will be displayed in the properties window.</li> <li>From these properties select copy url and paste it into a text editor. The workspace id is the first GUID in the URL, the lakehouse id is the second GUID in the URL.</li> <li>In the example below, the workspace id is <code>4f0cb887-047a-48a1-98c3-ebdb38c784c2</code> and the lakehouse id is <code>aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9</code>.</li> </ol> </li> </ol> </li> </ol> Example URL<pre><code>https://onelake.dfs.fabric.microsoft.com/4f0cb887-047a-48a1-98c3-ebdb38c784c2/aa2e5f92-53cc-4ab3-9a54-a6e5b1aeb9a9/Files/notebooks\n</code></pre>"},{"location":"user_guide/dbt_project_setup/#create-dbt-project","title":"Create Dbt Project","text":"<p>Once you have taken note of the workspace id, lakehouse id, workspace name and lakehouse name you can create a new dbt project and configure it to use the dbt-fabricsparknb adapter. To do this, run the code shown below:</p> <pre><code># Create your dbt project directories and profiles.yml file\ndbt init my_project # Note that the name of the project is arbitrary... call it whatever you like\n</code></pre> <p>When asked the questions below, provide the answers in bold below:</p> <ol> <li><code>Which data base would you like to use?</code> select <code>dbt-fabricksparknb</code></li> <li><code>Desired authentication method option (enter a number):</code> select <code>livy</code></li> <li><code>workspaceid (GUID of the workspace. Open the workspace from fabric.microsoft.com and copy the workspace url):</code> Enter the workspace id</li> <li><code>lakehouse (Name of the Lakehouse in the workspace that you want to connect to):</code> Enter the lakehouse name</li> <li><code>lakehouseid (GUID of the lakehouse, which can be extracted from url when you open lakehouse artifact from fabric.microsoft.com):</code> Enter the lakehouse id</li> <li><code>endpoint [https://api.fabric.microsoft.com/v1]:</code> Press enter to accept the default</li> <li><code>auth (Use CLI (az login) for interactive execution or SPN for automation) [CLI]:</code> select <code>cli</code></li> <li><code>client_id (Use when SPN auth is used.):</code> Enter a single space and press enter</li> <li><code>client_scrent (Use when SPN auth is used.):</code> Enter a single space and press enter</li> <li><code>tenant_id (Use when SPN auth is used.):</code> Enter a single space or Enter your PowerBI tenant id</li> <li><code>connect_retries [0]:</code> Enter 0</li> <li><code>connect_timeout [10]:</code> Enter 10</li> <li><code>schema (default schema that dbt will build objects in):</code> Enter <code>dbo</code></li> <li>threads (1 or more) [1]: Enter 1</li> </ol> <p>The command above will create a new directory called <code>my_project</code>. Within this directory you will find a <code>dbt_project.yml</code> file. Open this file in your favourite text editor and note that it should look like the example below except that in your case my_project will be replaced with the name of the project you created above.:</p> dbt_project.yml<pre><code># Name your project! Project names should contain only lowercase characters\n# and underscores. A good package name should reflect your organization's\n# name or the intended use of these models\nname: 'my_project'\nversion: '1.0.0'\n\n# This setting configures which \"profile\" dbt uses for this project.\nprofile: 'my_project'\n\n# These configurations specify where dbt should look for different types of files.\n# The `model-paths` config, for example, states that models in this project can be\n# found in the \"models/\" directory. You probably won't need to change these!\nmodel-paths: [\"models\"]\nanalysis-paths: [\"analyses\"]\ntest-paths: [\"tests\"]\nseed-paths: [\"seeds\"]\nmacro-paths: [\"macros\"]\nsnapshot-paths: [\"snapshots\"]\n\nclean-targets:         # directories to be removed by `dbt clean`\n  - \"target\"\n  - \"dbt_packages\"\n\n\n# Configuring models\n# Full documentation: https://docs.getdbt.com/docs/configuring-models\n\nmodels:\n  test4:\n    # Config indicated by + and applies to all files under models/example/\n    example:\n      +materialized: view\n</code></pre> <p>The dbt init command will also update your <code>profiles.yml</code> file with a profile matching your dbt project name. Open this file in your favourite text editor using the command below:</p> WindowsMacOSLinux <pre><code>code  $home/.dbt/profiles.yml\n</code></pre> <pre><code>code  ~/.dbt/profiles.yml\n</code></pre> <pre><code>code  ~/.dbt/profiles.yml\n</code></pre> <p>When run this will display a file similar to the one below. Check that your details are correct.</p> <p>Note</p> <p>The <code>profiles.yml</code> file should look like the example below except that in your case the highlighted lines may contain different values.</p> profiles.yml<pre><code>my_project:\n  target: my_project_target\n  outputs:\n    my_project_target:\n      authentication: CLI\n      method: livy\n      connect_retries: 0\n      connect_timeout: 10\n      endpoint: https://api.fabric.microsoft.com/v1\n      workspaceid: 4f0cb887-047a-48a1-98c3-ebdb38c784c2\n      workspacename: test\n      lakehousedatapath: /lakehouse\n      lakehouseid: 031feff6-071d-42df-818a-984771c083c4\n      lakehouse: datalake\n      schema: dbo\n      threads: 1\n      type: fabricsparknb\n      retry_all: true\n</code></pre> <p>Info</p> <p>You are now ready to move to the next step in which you will build your dbt project. Follow the Dbt Build Process guide.</p>"},{"location":"user_guide/development_workflow/","title":"Development Workflow","text":""},{"location":"user_guide/development_workflow/#development-and-deployment-flow-using-fabric-spark-notebook-adapter","title":"Development and Deployment Flow Using Fabric Spark Notebook Adapter","text":"<p>Advantages</p> <ul> <li> Available today</li> <li> Native Fabric Notebooks Generated and Deployed<ol> <li>Non dbt users able to view notebooks and business logic</li> <li>Monitoring and debugging of loads directly in Fabric without the need for a separate tool</li> </ol> </li> <li> Re-occurring loads achieved using native Fabric scheduling</li> <li> Simplified code promotion process using native Fabric Git integration</li> <li> No need for dbt hosted in a virtual machine<ol> <li>No need for service account</li> <li>No need for Azure Landing Zone</li> <li>No need for secure network connectivity between Azure VM and Fabric</li> </ol> </li> <li> Allows for disconnected development environment providing<ol> <li>Faster DBT build times</li> <li>Greater developer flexibility</li> </ol> </li> <li> Simplified code promotion Process using native Fabric Git integration<ol> <li>Single, native promotion process for all Fabric artifacts including non-dbt ones</li> </ol> </li> </ul> <p>Disadvantages</p> <ul> <li>Requires additional steps to extract metadata and generate notebooks but this is mitigated by our wrapper script that automates these.</li> </ul>"},{"location":"user_guide/development_workflow/#development-and-deployment-flow-using-original-fabric-spark-adapter","title":"Development and Deployment Flow Using Original Fabric Spark Adapter","text":""},{"location":"user_guide/development_workflow/#detailed-workflow","title":"Detailed Workflow","text":"<p>Inital Setup 1. Provision Workspace    - Development Environment: Fabric Portal    - Re-occurence: Do once per development environment set-up    - Instructions: Create a new workspace in the Power BI Portal, or use an existing workspace.</p> <ol> <li>Get Workspace Connection Details</li> <li>Development Environment: Fabric Portal</li> <li>Re-occurence: Do once per development environment set-up</li> <li> <p>Instructions: Get the workspace connection details from the Power BI Portal.</p> </li> <li> <p>Create or Update <code>profiles.yml</code></p> </li> <li>Development Environment: VS Code on local, developemnt machine</li> <li></li> <li> <p>Create or Update <code>dbt_project.yml</code> </p> </li> <li>Build Project</li> <li>Manually Upload Notebooks </li> <li>Run Meta Data Extract</li> </ol> <p>Ongoing Development Cycle</p> <ol> <li> <p>Download Metadata: </p> </li> <li> <p>Update Dbt Project </p> </li> <li>Build Dbt Project </li> <li>Verify Outputs </li> <li>Update Notebooks     <ol> <li>Upload to Onelake</li> <li>Update to GIT repo</li> </ol> </li> <li>Promote to Workspace     <ol> <li>Run Import Notebook</li> <li>Promote GIT branch</li> </ol> </li> <li>Run Master Notebook </li> <li>Validate Results </li> <li>Run Metadata Extract</li> </ol>"},{"location":"user_guide/fabric_ci_cd_process/","title":"Fabric CI/CD with Git Deployment","text":""},{"location":"user_guide/fabric_ci_cd_process/#git-based-deployment","title":"Git Based Deployment","text":"<p>The initial setup is based on a Git branch that is linked to all workspaces. As illustrated in the given example, we have described three stages: Development, Test, and Production. It also employs feature branches for individual developments within isolated workspaces using branch out functionality.</p> <p>The successful operation of this scenario depends on branching, merging, and pull requests.</p> <ol> <li><code>Each workspace is assigned its own branch.</code></li> <li><code>The introduction of new features is facilitated by raising pull requests.</code></li> <li><code>All deployments are initiated from the repository.</code></li> <li><code>To transition from Development to Test, and subsequently from Test to Production, a pull request must be initiated from the originating stage.</code></li> </ol> <p>The synchronization between the Git branch and the workspace can be automated. This is achieved by invoking the Git Sync API as part of a build pipelines, which is automatically triggered following the approval of a pull request.</p> <p></p>"},{"location":"user_guide/fabric_ci_cd_process/#git-and-build-environment-based-deployment","title":"Git and Build Environment Based Deployment","text":"<p>Git is exclusively linked to the Development workspace. The deployment to other stages is executed based on Build environments. This implies that the Fabric Item APIs are utilized to perform Create, Read, Update or Delete operations.</p> <p>Key points of this setup are:</p> <ol> <li><code>The Git repository serves as the foundation for creating, updating, or deleting items in the workspace.</code></li> <li><code>Git is solely connected to the Development workspace.</code></li> <li><code>Following a pull request, a Build pipeline is activated.</code></li> <li><code>The Build pipeline executes operations to the workspace.</code></li> </ol> <p>Note</p> <pre><code>This approach is code-intensive and for each future item to be supported, modifications may be required in the Build pipelines.\n</code></pre> <p></p>"},{"location":"user_guide/fabric_ci_cd_process/#git-and-fabric-deployment-pipeline-based-deployment","title":"Git and Fabric Deployment Pipeline Based Deployment","text":"<p>This is based on Fabric Deployment pipelines. This user-friendly interface simplifies the deployment process from one stage to another and is less code-intensive.</p> <p>Git is solely connected to the Development workspace, and feature branches continue to exist in separate workspaces. However, the Test, Production, and any additional workspaces are not linked to Git.</p> <p>Key aspects of this setup include:</p> <ol> <li><code>The release process to other stages, such as Test and Production, is managed via Deployment Pipelines in the Fabric.</code></li> <li><code>The Development workspace is the only one connected to Git.</code></li> <li><code>Triggers for the Fabric deployment pipeline can be automated. This is achieved by using Build Pipelines, which are automatically activated following the approval of a pull request.</code></li> </ol> <p>These pipelines can call the Fabric REST API and can also be integrated with Git Sync API for synchronizing the development workspace.</p> <p></p>"},{"location":"user_guide/generated_notebooks/","title":"Understanding the Generated Notebooks","text":""},{"location":"user_guide/generated_notebooks/#understanding-the-notebooks-generated","title":"Understanding the Notebooks Generated","text":"<p>When you run the build script successfully, you will see a series of notebooks generated in your my_project/target/notebooks directory. This is the <code>special sauce</code> of this dbt-adapter that allows your to run your dbt project natively as notebooks in a Fabric workspace. The image below shows a sample listing of generated notebooks. Your specific notebooks will be contain the name of your dbt project and may be different depending on the models and tests that you have defined in your dbt project.</p> <p>Sample listing of Generated Notebooks</p> <p></p> <p>If you study the files shown above you will notice that there is a naming convention and that the notebooks are prefixed with a specific string. The following table explains at a high level the naming convention and the purpose of each type of notebook.</p> Notebook Prefix Description model. These are dbt model notebooks. A notebook will be generated for each dbt model that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. test. These are dbt test notebooks. A notebook will be generated for each dbt test that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. seed. These are dbt seed notebooks. A notebook will be generated for each dbt seed that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt. master_ These are execution orchestration notebooks. They allow the running of your models, tests and seeds in parallel and in the correct order. They are what allow you to run your transformation pipelines independently of dbt as an orchestrator. In order to run your project simply schedule master.{project_name}.notebook.iypnb using Fabric's native scheduling functionality import_ This is a helper notebook that facilitate import of generated notebooks into workspace. metadata_ This is a helper notebook to facilitate generation of workspace metadata json files. <p>Important</p> <p>The green panels below provide a more detailed discussion of each type of notebook. Take a moment to expand each panel by clicking on it and read the detailed explanation of each type of notebook.</p> Notebooks with the Prefix <code>\"model.\"</code> <p>These are dbt model notebooks. A notebook will be generated for each dbt model that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt.</p> <p></p> <p></p> Notebooks with the Prefix <code>\"test.\"</code> <p>These are dbt test notebooks. A notebook will be generated for each dbt test that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt.</p> Notebooks with the Prefix <code>\"seed.\"</code> <p>These are dbt seed notebooks. A notebook will be generated for each dbt seed that you define. You will be able to run, debug and monitor execution of these notebooks directly in the Fabric portal independently of dbt.</p> Notebooks with the Prefix <code>\"master_\"</code> <p>These are execution orchestration notebooks. They allow the running of your models, tests and seeds in parallel and in the correct order. They are what allow you to run your transformation pipelines independently of dbt as an orchestrator. In order to run your project simply schedule master.{project_name}.notebook.iypnb using Fabric's native scheduling functionality.</p> Notebooks with the Prefix <code>\"import_\"</code> <p>This is a helper notebook that facilitates import of generated notebooks into workspace.</p> Notebooks with the Prefix <code>\"metadata_\"</code> <p>This is a helper notebook to facilitates the generation of workspace metadata json files.</p>"},{"location":"user_guide/generated_notebooks/#notebooks-in-your-fabric-workspace","title":"Notebooks in your Fabric Workspace","text":"<p>If you login to your fabric workspace and navigate to the notebooks section you will see that the generated notebooks have been uploaded to your workspace.</p> <p>Tip</p> <p>I suggest that you move your notebooks into a folder that matches the name of your dbt project.</p>"},{"location":"user_guide/initial_setup/","title":"Environment Setup","text":"<p>This section outlines the steps required to setup the development environment to use this dbt-adapter as part of a dbt data transformation project.</p> <p>To provide a common, cross-platform set of instructions we will first install Powershell. To facilitate the installation process we will use package managers such as winget for Windows, brew for MacOS and <code>apt</code> for Linux.</p>"},{"location":"user_guide/initial_setup/#core-tools-installation","title":"Core Tools Installation","text":"<p>Tip</p> <p>Following core tools can be installed using the standard powershell or in VS Code terminal (powershell)</p> WindowsMacOSLinux <pre><code># Winget Installs \nwinget install Microsoft.PowerShell\n</code></pre> <pre><code>brew install powershell/tap/powershell\n</code></pre> <pre><code># TBA\n</code></pre> <p>Next we will install Python and development tools such as vscode.</p> WindowsMacOSLinux <pre><code># Winget Installs \nwinget install Python.Python.3.12\nwinget install -e --id Microsoft.VisualStudioCode\nwinget install --id Git.Git -e --source winget\n\n# Python Environment Manager\nPython -m pip install --user virtualenv\n</code></pre> <pre><code># Brew Installs\nbrew install python@3.12\nbrew install --cask visual-studio-code\nbrew install git\n\n# Python Environment Manager\nPython -m pip install --user virtualenv\n</code></pre> <pre><code># TBA\n</code></pre>"},{"location":"user_guide/initial_setup/#other-tools","title":"Other tools","text":"<p>Now that we have pwsh installed, Make sure that you have install the following additional required tools.</p> <ul> <li>Install Azure PowerShell on Windows - Refer Azure Powershell Doc for windows (You might need to run this in admin mode)</li> </ul> <p>Important</p> <p>Optional packages you may need to install (Only run if you face issues)</p> <pre><code>pip config set global.trusted-host \"pypi.org files.pythonhosted.org pypi.python.org\"\n</code></pre>"},{"location":"user_guide/initial_setup/#source-directory-python-env","title":"Source Directory &amp; Python Env","text":"<p>Now lets create and activate our Python environment and install the required packages.</p> <p>Tip</p> <p>When executing the following, it can take a few minutes to complete on some machines. Occasionally pip may get stuck and in such cases break the execution using ctrl-c and run the same pip again. </p> WindowsMacOSLinux <pre><code># Ensure that you are in the pwsh shell\npwsh\n\n# Create a new source code directory\nmkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like\n# Navigate to the new directory\ncd dbt-fabricsparknb-test\n\n# Create the Python environment\npython -m venv .env\n\n#Optional step to run if activate.ps1 failes due to security policy\nSet-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope CurrentUser\n\n# Activate the Python environment\n./.env/Scripts/Activate.ps1\n\n# Install the dbt-fabricsparknb package from the repository\npip install --upgrade --force-reinstall git+https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb\n</code></pre> <pre><code># Ensure that you are in the pwsh shell\npwsh\n\n# Create a new source code directory\nmkdir dbt-fabricsparknb-test #Note that the name of the directory is arbitrary... call it whatever you like\n# Navigate to the new directory\ncd dbt-fabricsparknb-test\n\n# Create the Python environment\npython -m venv .env\n\n#Optional step to run if activate.ps1 failes due to security policy\nSet-ExecutionPolicy -ExecutionPolicy Unrestricted -Scope CurrentUser\n\n# Activate the Python environment\n./.env/Scripts/Activate.ps1  \n\n# Install the dbt-fabricsparknb package from the repository\npip install --upgrade --force-reinstall git+https://github.com/Insight-Services-APAC/APAC-Capability-DAI-DbtFabricSparkNb\n</code></pre> <pre><code># TBA\n</code></pre> <p>Info</p> <p>You are now ready to move to the next step in which you will set up your dbt project. Follow the Dbt Project Setup guide.</p>"}]}