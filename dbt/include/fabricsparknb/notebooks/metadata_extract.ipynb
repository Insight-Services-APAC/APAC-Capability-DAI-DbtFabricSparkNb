{"cells":[{"cell_type":"markdown","id":"70d877e7","metadata":{},"source":["[comment]: # (Attach Default Lakehouse Markdown Cell)\n","# üìå Attach Default Lakehouse\n","‚ùó**Note the code in the cell that follows is required to programatically attach the lakehouse and enable the running of spark.sql(). If this cell fails simply restart your session as this cell MUST be the first command executed on session start.**"]},{"cell_type":"code","execution_count":null,"id":"dce04845","metadata":{},"outputs":[],"source":["%%configure\n","{\n","    \"defaultLakehouse\": {  \n","        \"name\": \"{{lakehouse_name}}\",\n","    }\n","}"]},{"cell_type":"markdown","id":"2bdb5b27","metadata":{},"source":["# üì¶ Pip\n","Pip installs reqired specifically for this template should occur here"]},{"cell_type":"code","execution_count":null,"id":"07752518","metadata":{},"outputs":[],"source":["# No pip installs needed for this notebook"]},{"cell_type":"markdown","id":"11c9ff11","metadata":{},"source":["# üîó Imports"]},{"cell_type":"code","execution_count":null,"id":"fe19e753","metadata":{},"outputs":[],"source":["from notebookutils import mssparkutils # type: ignore\n","from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n","from pyspark.sql.functions import lower, udf, col, row_number, when, monotonically_increasing_id\n","from pyspark.sql.window import Window\n","from concurrent.futures import ThreadPoolExecutor, as_completed\n","from dataclasses import dataclass\n","import json\n","import hashlib\n","import yaml\n","import re\n"]},{"cell_type":"markdown","id":"0013b888-1464-4664-a1f4-d382d141ca44","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# { } Params"]},{"cell_type":"code","execution_count":53,"id":"a5e27e4a-df79-4ac0-a338-1dda0ac03f4c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-04-27T02:52:24.0250181Z","execution_start_time":"2024-04-27T02:52:23.7665395Z","livy_statement_state":"available","parent_msg_id":"690af652-34e3-4abf-bdac-9edb238ab97f","queued_time":"2024-04-27T02:52:23.0985456Z","session_id":"79594e22-45e2-4587-a343-c5b3f00bc4fb","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":55,"statement_ids":[55]},"text/plain":["StatementMeta(, 79594e22-45e2-4587-a343-c5b3f00bc4fb, 55, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["# Set Lakehouse\n","RelativePathForMetaData = \"Files/metaextracts/\""]},{"cell_type":"markdown","id":"242ad5a9","metadata":{},"source":["# #Ô∏è‚É£ Functions"]},{"cell_type":"code","execution_count":54,"id":"24ad9d35-b433-4e11-86f3-4fce5a017d80","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-04-27T02:52:24.7167997Z","execution_start_time":"2024-04-27T02:52:24.4727615Z","livy_statement_state":"available","parent_msg_id":"52e68895-2c4e-49e8-8497-a2a85c0608cf","queued_time":"2024-04-27T02:52:23.1511212Z","session_id":"79594e22-45e2-4587-a343-c5b3f00bc4fb","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":56,"statement_ids":[56]},"text/plain":["StatementMeta(, 79594e22-45e2-4587-a343-c5b3f00bc4fb, 56, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["def execute_sql_and_write_to_file(sql_query, file_path):\n","    # Execute the SQL query\n","    df = spark.sql(sql_query) # type: ignore\n","    df_to_file(df, file_path)\n","\n","def df_to_file(df, file_path):\n","\n","    # Convert the DataFrame to a Pandas DataFrame\n","    pandas_df = df.toPandas()\n","\n","    # Convert the Pandas DataFrame to a string\n","    df_string = pandas_df.to_json(None, orient='records')\n","\n","    # Write the string to the file\n","    mssparkutils.fs.put(file_path, df_string, True) # type: ignore\n"]},{"cell_type":"markdown","id":"bb27a6ea-2a9f-44df-a955-b4d224fa50ff","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# List Schemas Macro Export"]},{"cell_type":"code","execution_count":55,"id":"7e0c30c6-b3c3-45ce-a3b2-9ce0148f6d32","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-04-27T02:52:27.5580209Z","execution_start_time":"2024-04-27T02:52:25.1256841Z","livy_statement_state":"available","parent_msg_id":"216be99b-ea49-44cd-8f7c-17cbdc225101","queued_time":"2024-04-27T02:52:23.2196536Z","session_id":"79594e22-45e2-4587-a343-c5b3f00bc4fb","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":57,"statement_ids":[57]},"text/plain":["StatementMeta(, 79594e22-45e2-4587-a343-c5b3f00bc4fb, 57, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["#convert database/ lakehouse name to lower case\n","#execute_sql_and_write_to_file(\"show databases\", f\"{RelativePathForMetaData}/ListSchemas.json\")\n","df_database = spark.sql(\"show databases\")\n","df_database = df_database.withColumn('namespace', lower(df_database['namespace']))\n","\n","df_to_file (df_database, f\"{RelativePathForMetaData}/ListSchemas.json\")"]},{"cell_type":"markdown","id":"e69ab86b-f406-4514-84ef-3f7491fee9e2","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# List Relations"]},{"cell_type":"code","execution_count":56,"id":"bd001abf-804c-41d5-8681-2c39e88585c2","metadata":{"collapsed":false,"microsoft":{}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-04-27T02:52:36.0489538Z","execution_start_time":"2024-04-27T02:52:28.0456844Z","livy_statement_state":"available","parent_msg_id":"b9553cd2-389d-4a26-83a2-de73ed1757d7","queued_time":"2024-04-27T02:52:23.2910662Z","session_id":"79594e22-45e2-4587-a343-c5b3f00bc4fb","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":58,"statement_ids":[58]},"text/plain":["StatementMeta(, 79594e22-45e2-4587-a343-c5b3f00bc4fb, 58, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["# Define the schema\n","schema = StructType([\n","    StructField(\"namespace\", StringType(), True),\n","    StructField(\"tableName\", StringType(), True),\n","    StructField(\"isTemporary\", BooleanType(), True),\n","    StructField(\"information\", StringType(), True),\n","    StructField(\"type\", StringType(), True)\n","])\n","\n","# Create an empty DataFrame with the schema\n","union_df = spark.createDataFrame([], schema) # type: ignore\n","\n","\n","# Define a UDF to extract the Type from metadata\n","def extract_type(metadata):\n","    match = re.search(r\"Type:\\s*(\\w+)\", metadata)\n","    if match:\n","        return match.group(1)\n","    return None\n","\n","extract_type_udf = udf(extract_type, StringType())\n","\n","# Function to execute SQL operations\n","def execute_sql_operations(namespace):\n","    sql = f\"show table extended in {namespace} like '*'\"\n","    df_temp = spark.sql(sql)  # type: ignore\n","    df_temp = df_temp.withColumn(\"type\", extract_type_udf(df_temp[\"information\"]))\n","    \n","    return df_temp\n","\n","\n","# Run SQL operations in parallel using ThreadPoolExecutor\n","def run_sql_operations_in_parallel(namespaces):\n","    results = []\n","    with ThreadPoolExecutor() as executor:\n","        future_to_namespace = {executor.submit(execute_sql_operations, row['namespace']): row['namespace'] for row in namespaces}\n","        for future in as_completed(future_to_namespace):\n","            namespace = future_to_namespace[future]\n","            try:\n","                df_temp = future.result()\n","                results.append(df_temp)\n","            except Exception as exc:\n","                print(f'{namespace} generated an exception: {exc}')\n","    return results\n","\n","df = spark.sql(\"show databases\") # type: ignore\n","namespaces = df.collect()\n","\n","# Run SQL operations in parallel and collect results\n","results = run_sql_operations_in_parallel(namespaces)\n","\n","# Union all DataFrames\n","for df_temp in results:\n","    union_df = union_df.union(df_temp)\n","\n","# Lowercasing removed as it causes issues with describe statements eg. describe unable to find objects. ## John Rampono: 11-Sep-24\n","#convert database/ lakehouse name and table name to lower case\n","# union_df = union_df.withColumn('namespace', lower(union_df['namespace']))\n","# union_df = union_df.withColumn('tableName', lower(union_df['tableName']))\n","\n","# Remove noise \n","union_df = union_df.filter((col(\"namespace\") != \"\") & (col(\"namespace\").isNotNull()))\n","\n","df_to_file(union_df, f\"{RelativePathForMetaData}/ListRelations.json\")"]},{"cell_type":"markdown","id":"f9f81007-df29-4388-968c-2c16f7066021","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Describe Table Extended"]},{"cell_type":"code","execution_count":60,"id":"ac47579c-c536-4150-8160-bad02f59e534","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-04-27T03:12:26.4101893Z","execution_start_time":"2024-04-27T03:12:12.0299927Z","livy_statement_state":"available","parent_msg_id":"9da09a4b-0727-4dcf-9938-89588d1c00dc","queued_time":"2024-04-27T03:12:11.5311229Z","session_id":"79594e22-45e2-4587-a343-c5b3f00bc4fb","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":62,"statement_ids":[62]},"text/plain":["StatementMeta(, 79594e22-45e2-4587-a343-c5b3f00bc4fb, 62, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["import json\n","from pyspark.sql.types import StructType, StructField, StringType, BooleanType # type: ignore\n","from pyspark.sql.functions import lit # type: ignore\n","\n","\n","# Define the schema\n","schema = StructType([\n","    StructField(\"col_name\", StringType(), True),\n","    StructField(\"data_type\", StringType(), True),\n","    StructField(\"comment\", StringType(), True), \n","    StructField(\"namespace\", StringType(), True),\n","    StructField(\"tableName\", StringType(), True),\n","])\n","\n","\n","# Create an empty DataFrame with the schema\n","union_df = spark.createDataFrame([], schema) # type: ignore\n","union_dtable = spark.createDataFrame([], schema) # type: ignore\n","\n","\n","\n","# Define a function to execute the SQL operations\n","def execute_sql_operations(j):\n","    # Use statement swapped for two part naming as it did not appear to be working ## John Rampono: 11-Sep-24\n","    # sql = f\"use {j['namespace']}\"\n","    # spark.sql(sql)  # type: ignore\n","    sql = f\"describe extended {j['namespace']}.{j['tableName']}\"\n","    df_temp = spark.sql(sql)  # type: ignore\n","        \n","    # Add a row number column using monotonically_increasing_id\n","    df_with_row_num = df_temp.withColumn(\"row_num\", monotonically_increasing_id())\n","\n","    # Identify the first row where col_name is null or empty\n","    first_null_or_empty_row = df_with_row_num.filter((col(\"col_name\") == \"\") | col(\"col_name\").isNull()).orderBy(\"row_num\").limit(1)\n","\n","    # Get the row number of the first null or empty row\n","    first_null_or_empty_row_num = first_null_or_empty_row.select(\"row_num\").collect()\n","\n","    if first_null_or_empty_row_num:\n","        first_null_or_empty_row_num = first_null_or_empty_row_num[0][\"row_num\"]\n","        # Filter the DataFrame to keep only rows before this row\n","        df_filtered = df_with_row_num.filter(col(\"row_num\") < first_null_or_empty_row_num).drop(\"row_num\")\n","    else:\n","        # If no null or empty row is found, keep the original DataFrame\n","        df_filtered = df_temp    \n","    \n","    df_filtered = df_filtered.withColumn('namespace', lit(j['namespace']))\n","    df_filtered = df_filtered.withColumn('tableName', lit(j['tableName']))\n","\n","    # generate Schema.yml\n","    sqldesc = f\"describe {j['namespace']}.{j['tableName']}\"\n","    df_dtable = spark.sql(sqldesc)  # type: ignore\n","    \n","    # Lowercasing removed as it causes issues with describe statements eg. describe unable to find objects. ## John Rampono: 11-Sep-24\n","    df_dtable = df_dtable.withColumn('namespace', lit(j['namespace']))\n","    df_dtable = df_dtable.withColumn('tableName', lit(j['tableName']))\n","\n","    return df_filtered, df_dtable\n","\n","# Run SQL operations in parallel using ThreadPoolExecutor\n","def run_sql_operations_in_parallel(jo):\n","    union_df = None\n","    union_dtable = None\n","\n","    with ThreadPoolExecutor() as executor:\n","        future_to_sql = {executor.submit(execute_sql_operations, j): j for j in jo}\n","        for future in as_completed(future_to_sql):\n","            j = future_to_sql[future]\n","            try:\n","                df_temp, df_dtable = future.result()\n","                if union_df is None:\n","                    union_df = df_temp\n","                else:\n","                    union_df = union_df.union(df_temp)\n","\n","                if union_dtable is None:\n","                    union_dtable = df_dtable\n","                else:\n","                    union_dtable = union_dtable.union(df_dtable)\n","            except Exception as exc:\n","                tableName = j['tableName']\n","                namespace = j['namespace']\n","                print(f'{namespace}.{tableName} generated an exception: {exc}')\n","\n","    return union_df, union_dtable\n","\n","\n","data = spark.sparkContext.wholeTextFiles(f\"{RelativePathForMetaData}/ListRelations.json\").collect() # type: ignore\n","file_content = data[0][1]\n","jo = json.loads(file_content)\n","\n","# Filter out views as these will cause issues  ## John Rampono: 11-Sep-24\n","filtered_jo = [item for item in jo if item[\"type\"].lower() == \"managed\"]\n","\n","union_df, union_dtable = run_sql_operations_in_parallel(filtered_jo)\n","\n","# Lowercasing removed as it causes issues with describe statements eg. describe unable to find objects. ## John Rampono: 11-Sep-24\n","#convert column name to lower case\n","#union_df = union_df.withColumn('col_name', lower(union_df['col_name']))\n","#union_dtable = union_dtable.withColumn('col_name', lower(union_dtable['col_name']))\n","\n","# Do we still need this view?\n","union_dtable.createOrReplaceTempView(\"describetable\")\n","\n","#create file\n","df_to_file(union_df, f\"{RelativePathForMetaData}/DescribeRelations.json\")"]},{"cell_type":"markdown","id":"9eb9e644","metadata":{},"source":["# Schema yaml generation"]},{"cell_type":"code","execution_count":null,"id":"74deca63","metadata":{},"outputs":[],"source":["tables = {}\n","for item in union_dtable.collect():\n","    #print(item)\n","    table_name = item['tableName']\n","    col_name = item['col_name']\n","    \n","    if table_name not in tables:\n","        tables[table_name] = []\n","    \n","    if col_name:  # Only add columns with a name\n","        tables[table_name].append({\n","            \"name\": col_name,\n","            \"description\": \"add column description\"\n","        })\n","\n","output_data = {}\n","output_data[\"version\"] = 2\n","models = []\n","for table_name, columns in tables.items():\n","    models.append({\n","        \"name\": table_name,\n","        \"description\": \"add table description\",\n","        \"columns\": columns\n","    })\n","output_data[\"models\"] = models\n","\n","yaml_string = yaml.dump(output_data,  default_flow_style=False)\n","RelativePathForShemaTemplate = \"Files/SchemaTemplate\"\n","file_path = f\"{RelativePathForShemaTemplate}/schema.yml\"\n","mssparkutils.fs.put(file_path, yaml_string, True) # type: ignore"]},{"cell_type":"markdown","id":"683a2a2f","metadata":{},"source":["# #Ô∏è‚É£ Create Hash for Files "]},{"cell_type":"code","execution_count":null,"id":"92458c21","metadata":{},"outputs":[],"source":["# Define the schema\n","schema = StructType([\n","    StructField(\"hash\", StringType(), True),\n","    StructField(\"file\", StringType(), True)\n","])\n","\n","files = [\"ListSchemas\",\"DescribeRelations\",\"ListRelations\"]\n","\n","df = spark.createDataFrame([], schema) # type: ignore\n","for file in files:\n","    data = spark.sparkContext.wholeTextFiles(f\"{RelativePathForMetaData}/{file}.json\").collect() # type: ignore\n","    file_content = data[0][1]\n","    hashinfo = {}\n","    hashinfo['hash'] = hashlib.sha256(file_content.encode('utf-8')).hexdigest()\n","    hashinfo['file'] = f\"{file}.json\"\n","    df = df.union(spark.createDataFrame([hashinfo], schema)) # type: ignore\n","    \n","df_to_file(df, f\"{RelativePathForMetaData}/MetaHashes.json\") # type: ignore\n"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"031feff6-071d-42df-818a-984771c083c4","default_lakehouse_name":"DataLake","default_lakehouse_workspace_id":"4f0cb887-047a-48a1-98c3-ebdb38c784c2"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
