{"cells":[{"cell_type":"markdown","id":"2bdb5b27","metadata":{},"source":["# üì¶ Pip\n","Pip installs reqired specifically for this template should occur here"]},{"cell_type":"code","execution_count":null,"id":"07752518","metadata":{},"outputs":[],"source":["# No pip installs needed for this notebook"]},{"cell_type":"markdown","id":"11c9ff11","metadata":{},"source":["# üîó Imports"]},{"cell_type":"code","execution_count":null,"id":"fe19e753","metadata":{},"outputs":[],"source":["from notebookutils import mssparkutils # type: ignore\n","from pyspark.sql.types import StructType, StructField, StringType, BooleanType\n","from pyspark.sql.functions import lower\n","from dataclasses import dataclass\n","import json\n","import hashlib\n","import yaml\n"]},{"cell_type":"markdown","id":"0013b888-1464-4664-a1f4-d382d141ca44","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# { } Params"]},{"cell_type":"code","execution_count":53,"id":"a5e27e4a-df79-4ac0-a338-1dda0ac03f4c","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-04-27T02:52:24.0250181Z","execution_start_time":"2024-04-27T02:52:23.7665395Z","livy_statement_state":"available","parent_msg_id":"690af652-34e3-4abf-bdac-9edb238ab97f","queued_time":"2024-04-27T02:52:23.0985456Z","session_id":"79594e22-45e2-4587-a343-c5b3f00bc4fb","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":55,"statement_ids":[55]},"text/plain":["StatementMeta(, 79594e22-45e2-4587-a343-c5b3f00bc4fb, 55, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["# Set Lakehouse\n","RelativePathForMetaData = \"Files/MetaExtracts/\""]},{"cell_type":"markdown","id":"242ad5a9","metadata":{},"source":["# #Ô∏è‚É£ Functions"]},{"cell_type":"code","execution_count":54,"id":"24ad9d35-b433-4e11-86f3-4fce5a017d80","metadata":{"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-04-27T02:52:24.7167997Z","execution_start_time":"2024-04-27T02:52:24.4727615Z","livy_statement_state":"available","parent_msg_id":"52e68895-2c4e-49e8-8497-a2a85c0608cf","queued_time":"2024-04-27T02:52:23.1511212Z","session_id":"79594e22-45e2-4587-a343-c5b3f00bc4fb","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":56,"statement_ids":[56]},"text/plain":["StatementMeta(, 79594e22-45e2-4587-a343-c5b3f00bc4fb, 56, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["def execute_sql_and_write_to_file(sql_query, file_path):\n","    # Execute the SQL query\n","    df = spark.sql(sql_query) # type: ignore\n","    df_to_file(df, file_path)\n","\n","def df_to_file(df, file_path):\n","\n","    # Convert the DataFrame to a Pandas DataFrame\n","    pandas_df = df.toPandas()\n","\n","    # Convert the Pandas DataFrame to a string\n","    df_string = pandas_df.to_json(None, orient='records')\n","\n","    # Write the string to the file\n","    mssparkutils.fs.put(file_path, df_string, True) # type: ignore\n"]},{"cell_type":"markdown","id":"bb27a6ea-2a9f-44df-a955-b4d224fa50ff","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# List Schemas Macro Export"]},{"cell_type":"code","execution_count":55,"id":"7e0c30c6-b3c3-45ce-a3b2-9ce0148f6d32","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"microsoft":{},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-04-27T02:52:27.5580209Z","execution_start_time":"2024-04-27T02:52:25.1256841Z","livy_statement_state":"available","parent_msg_id":"216be99b-ea49-44cd-8f7c-17cbdc225101","queued_time":"2024-04-27T02:52:23.2196536Z","session_id":"79594e22-45e2-4587-a343-c5b3f00bc4fb","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":57,"statement_ids":[57]},"text/plain":["StatementMeta(, 79594e22-45e2-4587-a343-c5b3f00bc4fb, 57, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["#convert database/ lakehouse name to lower case\n","#execute_sql_and_write_to_file(\"show databases\", f\"{RelativePathForMetaData}/ListSchemas.json\")\n","df_database = spark.sql(\"show databases\")\n","df_database = df_database.withColumn('namespace', lower(df_database['namespace']))\n","\n","df_to_file (df_database, f\"{RelativePathForMetaData}/ListSchemas.json\")"]},{"cell_type":"markdown","id":"e69ab86b-f406-4514-84ef-3f7491fee9e2","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# List Relations"]},{"cell_type":"code","execution_count":56,"id":"bd001abf-804c-41d5-8681-2c39e88585c2","metadata":{"collapsed":false,"microsoft":{}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-04-27T02:52:36.0489538Z","execution_start_time":"2024-04-27T02:52:28.0456844Z","livy_statement_state":"available","parent_msg_id":"b9553cd2-389d-4a26-83a2-de73ed1757d7","queued_time":"2024-04-27T02:52:23.2910662Z","session_id":"79594e22-45e2-4587-a343-c5b3f00bc4fb","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":58,"statement_ids":[58]},"text/plain":["StatementMeta(, 79594e22-45e2-4587-a343-c5b3f00bc4fb, 58, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["# Define the schema\n","schema = StructType([\n","    StructField(\"namespace\", StringType(), True),\n","    StructField(\"tableName\", StringType(), True),\n","    StructField(\"isTemporary\", BooleanType(), True),\n","    StructField(\"information\", StringType(), True)\n","])\n","\n","# Create an empty DataFrame with the schema\n","union_df = spark.createDataFrame([], schema) # type: ignore\n","\n","df = spark.sql(\"show databases\") # type: ignore\n","for row in df.collect():\n","    sql = f\"show table extended in {row['namespace']} like '*'\"\n","    df_temp = spark.sql(sql) # type: ignore\n","    \n","    #display(df_temp)\n","    union_df = union_df.union(df_temp)\n","\n","#convert database/ lakehouse name and table name to lower case\n","union_df = union_df.withColumn('namespace', lower(union_df['namespace']))\n","union_df = union_df.withColumn('tableName', lower(union_df['tableName']))\n","\n","df_to_file(union_df, f\"{RelativePathForMetaData}/ListRelations.json\")"]},{"cell_type":"markdown","id":"f9f81007-df29-4388-968c-2c16f7066021","metadata":{"nteract":{"transient":{"deleting":false}}},"source":["# Describe Table Extended"]},{"cell_type":"code","execution_count":60,"id":"ac47579c-c536-4150-8160-bad02f59e534","metadata":{"collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":false},"nteract":{"transient":{"deleting":false}}},"outputs":[{"data":{"application/vnd.livy.statement-meta+json":{"execution_finish_time":"2024-04-27T03:12:26.4101893Z","execution_start_time":"2024-04-27T03:12:12.0299927Z","livy_statement_state":"available","parent_msg_id":"9da09a4b-0727-4dcf-9938-89588d1c00dc","queued_time":"2024-04-27T03:12:11.5311229Z","session_id":"79594e22-45e2-4587-a343-c5b3f00bc4fb","session_start_time":null,"spark_pool":null,"state":"finished","statement_id":62,"statement_ids":[62]},"text/plain":["StatementMeta(, 79594e22-45e2-4587-a343-c5b3f00bc4fb, 62, Finished, Available)"]},"metadata":{},"output_type":"display_data"}],"source":["import json\n","from pyspark.sql.types import StructType, StructField, StringType, BooleanType # type: ignore\n","from pyspark.sql.functions import lit # type: ignore\n","\n","\n","# Define the schema\n","schema = StructType([\n","    StructField(\"col_name\", StringType(), True),\n","    StructField(\"data_type\", StringType(), True),\n","    StructField(\"comment\", StringType(), True), \n","    StructField(\"namespace\", StringType(), True),\n","    StructField(\"tableName\", StringType(), True),\n","])\n","\n","\n","# Create an empty DataFrame with the schema\n","union_df = spark.createDataFrame([], schema) # type: ignore\n","union_dtable = spark.createDataFrame([], schema) # type: ignore\n","\n","data = spark.sparkContext.wholeTextFiles(f\"{RelativePathForMetaData}/ListRelations.json\").collect() # type: ignore\n","file_content = data[0][1]\n","jo = json.loads(file_content)\n","for j in jo:\n","    sql = f\"use { j['namespace'] }\"\n","    spark.sql(sql) # type: ignore\n","    sql = f\"describe extended { j['tableName'] }\"\n","    df_temp = spark.sql(sql) # type: ignore\n","    \n","    df_temp = df_temp.withColumn('namespace', lit(j['namespace']))\n","    df_temp = df_temp.withColumn('tableName', lit(j['tableName']))\n","    \n","    union_df = union_df.union(df_temp)\n","    \n","    # generate Schema.yml\n","    sqldesc = f\"describe { j['tableName'] }\"\n","    df_dtable = spark.sql(sqldesc) # type: ignore\n","    df_dtable = df_dtable.withColumn('namespace', lower(lit(j['namespace'])))\n","    df_dtable = df_dtable.withColumn('tableName', lower(lit(j['tableName'])))\n","    union_dtable = union_dtable.union(df_dtable)\n","\n","#convert column name to lower case\n","union_df = union_df.withColumn('col_name', lower(union_df['col_name']))\n","union_dtable = union_dtable.withColumn('col_name', lower(union_dtable['col_name']))\n","union_dtable.createOrReplaceTempView(\"describetable\")\n","df_to_file(union_df, f\"{RelativePathForMetaData}/DescribeRelations.json\")"]},{"cell_type":"markdown","id":"9eb9e644","metadata":{},"source":["# Schema yaml generation"]},{"cell_type":"code","execution_count":null,"id":"74deca63","metadata":{},"outputs":[],"source":["tables = {}\n","for item in union_dtable.collect():\n","    #print(item)\n","    table_name = item['tableName']\n","    col_name = item['col_name']\n","    \n","    if table_name not in tables:\n","        tables[table_name] = []\n","    \n","    if col_name:  # Only add columns with a name\n","        tables[table_name].append({\n","            \"name\": col_name,\n","            \"description\": \"add column description\"\n","        })\n","\n","output_data = {}\n","output_data[\"version\"] = 2\n","models = []\n","for table_name, columns in tables.items():\n","    models.append({\n","        \"name\": table_name,\n","        \"description\": \"add table description\",\n","        \"columns\": columns\n","    })\n","output_data[\"models\"] = models\n","\n","yaml_string = yaml.dump(output_data,  default_flow_style=False)\n","RelativePathForShemaTemplate = \"Files/SchemaTemplate\"\n","file_path = f\"{RelativePathForShemaTemplate}/schema.yml\"\n","mssparkutils.fs.put(file_path, yaml_string, True) # type: ignore"]},{"cell_type":"markdown","id":"683a2a2f","metadata":{},"source":["# #Ô∏è‚É£ Create Hash for Files "]},{"cell_type":"code","execution_count":null,"id":"92458c21","metadata":{},"outputs":[],"source":["# Define the schema\n","schema = StructType([\n","    StructField(\"hash\", StringType(), True),\n","    StructField(\"file\", StringType(), True)\n","])\n","\n","files = [\"ListSchemas\",\"DescribeRelations\",\"ListRelations\"]\n","\n","df = spark.createDataFrame([], schema) # type: ignore\n","for file in files:\n","    data = spark.sparkContext.wholeTextFiles(f\"{RelativePathForMetaData}/{file}.json\").collect() # type: ignore\n","    file_content = data[0][1]\n","    hashinfo = {}\n","    hashinfo['hash'] = hashlib.sha256(file_content.encode('utf-8')).hexdigest()\n","    hashinfo['file'] = f\"{file}.json\"\n","    df = df.union(spark.createDataFrame([hashinfo], schema)) # type: ignore\n","    \n","df_to_file(df, f\"{RelativePathForMetaData}/MetaHashes.json\") # type: ignore\n"]}],"metadata":{"dependencies":{"lakehouse":{"default_lakehouse":"031feff6-071d-42df-818a-984771c083c4","default_lakehouse_name":"DataLake","default_lakehouse_workspace_id":"4f0cb887-047a-48a1-98c3-ebdb38c784c2"}},"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default"},"synapse_widget":{"state":{},"version":"0.1"},"widgets":{}},"nbformat":4,"nbformat_minor":5}
