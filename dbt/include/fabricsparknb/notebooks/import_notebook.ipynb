{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Attach Default Lakehouse\n",
    "â—**Note the code in the cell that follows is required to programatically attach the lakehouse and enable the running of spark.sql(). If this cell fails simply restart your session as this cell MUST be the first command executed on session start.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"defaultLakehouse\": {  \n",
    "        \"name\": \"{{lakehouse_name}}\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“¦ Pip\n",
    "Pip installs reqired specifically for this template should occur here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No pip installs needed for this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”— Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils # type: ignore\n",
    "from datetime import datetime\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import base64\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #ï¸âƒ£ Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_file_content_using_notebookutils(file):\n",
    "    \"\"\"Get the content of a file using notebookutils.\"\"\"\n",
    "    #return self.mssparkutils.fs.head(file, 1000000000)\n",
    "    data = spark.sparkContext.wholeTextFiles(file).collect() # type: ignore\n",
    "\n",
    "    # data is a list of tuples, where the first element is the file path and the second element is the content of the file\n",
    "    file_content = data[0][1]\n",
    "\n",
    "    return file_content\n",
    "\n",
    "\n",
    "def update_notebook(ntbk_name, ntbk_json, workspace_id, ntbk_id):\n",
    "\n",
    "        api_endpoint = \"api.fabric.microsoft.com\"\n",
    "        pbi_token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api') \n",
    "\n",
    "        print(f\"Updating '{ntbk_name}'...\")\n",
    "        url = f\"https://{api_endpoint}/v1/workspaces/{workspace_id}/items/{ntbk_id}/updateDefinition\"\n",
    "\n",
    "        json_str = json.dumps(ntbk_json)\n",
    "        json_bytes = json_str.encode('utf-8')\n",
    "        base64_encoded_json = base64.b64encode(json_bytes)\n",
    "        base64_str = base64_encoded_json.decode('utf-8')\n",
    "        print('payload')\n",
    "        payload = json.dumps({\n",
    "            \"definition\" : {\n",
    "                \"format\": \"ipynb\",\n",
    "                \"parts\" : [\n",
    "                    {\n",
    "                        \"path\": \"notebook-content.ipynb\",\n",
    "                        \"payload\": base64_str,\n",
    "                        \"payloadType\": \"InlineBase64\"\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        })\n",
    "\n",
    "        headers = {\n",
    "            'Authorization': f'Bearer {pbi_token}',\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "        if response.ok:\n",
    "            print(f\">> Notebook '{ntbk_name}' updated.\")\n",
    "        else:\n",
    "            raise RuntimeError(f\"Notebook '{ntbk_name}' update failed: {response.status_code}: {response.text}\")\n",
    "\n",
    "\n",
    "def import_notebooks(output_folder, workspace_id, notebook_names=None):\n",
    "        date = datetime.now().strftime('%Y_%m_%dT%H_%M_%S')\n",
    "        resource_type = \"notebooks\"\n",
    "        res_imported = 0\n",
    "        resources_imported = {}\n",
    "        existing_nb_id = None\n",
    "        artifact_path = f\"{output_folder}\"\n",
    "\n",
    "        if not mssparkutils.fs.exists(artifact_path):\n",
    "            print(f\"Path where the import artifacts from Synapse are located {artifact_path} does not exist. Exiting ...\")\n",
    "            return\n",
    "\n",
    "        print(f\"Importing individual resources of type '{resource_type}' into Fabric workspace '{workspace_id}'...\")\n",
    "        notebook_names_raw = [f.name for f in mssparkutils.fs.ls(artifact_path)]\n",
    "        notebook_names = [name.replace('.ipynb','') for name in notebook_names_raw if name.endswith(\".ipynb\")]\n",
    "        print(notebook_names)\n",
    "        existing_nbs = get_notebooks(workspace_id)\n",
    "\n",
    "        for notebook_name in notebook_names:\n",
    "            time.sleep(2)\n",
    "            existing_nb = [nb for nb in existing_nbs if nb['displayName'] == notebook_name ]\n",
    "            if len(existing_nb) == 1:\n",
    "                print(f'{notebook_name} exists already... updating...')\n",
    "                existing_nb_id = existing_nb[0]['id'] \n",
    "            \n",
    "            file_path = os.path.join(artifact_path, f\"{notebook_name}.ipynb\")\n",
    "            if mssparkutils.fs.exists(file_path):\n",
    "                read_file = get_file_content_using_notebookutils(file_path)\n",
    "                ntbk_json = json.loads(read_file)\n",
    "                ntbk_name = f\"{notebook_name}\"\n",
    "                if existing_nb_id == None:\n",
    "                    import_notebook(ntbk_name, ntbk_json, workspace_id, False)\n",
    "                else:\n",
    "                    update_notebook(ntbk_name, ntbk_json, workspace_id, existing_nb_id)\n",
    "                res_imported += 1\n",
    "\n",
    "        resources_imported[resource_type] = res_imported\n",
    "        print(f\"Finish importing {res_imported} items of type: {resource_type}\")\n",
    "\n",
    "def import_notebook(ntbk_name, ntbk_json, workspace_id, overwrite=False):\n",
    "\n",
    "    api_endpoint = \"api.fabric.microsoft.com\"\n",
    "    pbi_token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api') \n",
    "\n",
    "    print(f\"Importing '{ntbk_name}'...\")\n",
    "    url = f\"https://{api_endpoint}/v1/workspaces/{workspace_id}/items\"\n",
    "\n",
    "    json_str = json.dumps(ntbk_json)\n",
    "    json_bytes = json_str.encode('utf-8')\n",
    "    base64_encoded_json = base64.b64encode(json_bytes)\n",
    "    base64_str = base64_encoded_json.decode('utf-8')\n",
    "\n",
    "    payload = json.dumps({\n",
    "        \"type\": \"Notebook\",\n",
    "        \"description\": \"Imported from Synapse\",\n",
    "        \"displayName\": ntbk_name,\n",
    "        \"definition\" : {\n",
    "            \"format\": \"ipynb\",\n",
    "            \"parts\" : [\n",
    "                {\n",
    "                    \"path\": \"notebook-content.ipynb\",\n",
    "                    \"payload\": base64_str,\n",
    "                    \"payloadType\": \"InlineBase64\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    })\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {pbi_token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "    if response.ok:\n",
    "        print(f\">> Notebook '{ntbk_name}' created.\")\n",
    "    else:\n",
    "        raise RuntimeError(f\"Notebook '{ntbk_name}' creation failed: {response.status_code}: {response.text}\")\n",
    "\n",
    "\n",
    "def get_notebooks(workspace_id):\n",
    "    api_endpoint = \"api.fabric.microsoft.com\"\n",
    "    pbi_token = mssparkutils.credentials.getToken('https://analysis.windows.net/powerbi/api') \n",
    "\n",
    "    url = f\"https://{api_endpoint}/v1/workspaces/{workspace_id}/items?type=Notebook\"\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {pbi_token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    response = requests.request(\"GET\", url, headers=headers)\n",
    "\n",
    "    if response.ok:        \n",
    "        nbs = []\n",
    "        for rr in response.json()['value']:\n",
    "            nb = {}\n",
    "            nb['id'] = rr['id'] \n",
    "            nb['displayName'] = rr['displayName']\n",
    "            nb['matched'] = False\n",
    "            nbs.append(nb)\n",
    "        return nbs\n",
    "    else:\n",
    "        raise RuntimeError(f\"Notebook '{ntbk_name}' creation failed: {response.status_code}: {response.text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_id = spark.conf.get(\"trident.workspace.id\") # type: ignore\n",
    "import_notebooks(f\"Files/notebooks\", workspace_id)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
