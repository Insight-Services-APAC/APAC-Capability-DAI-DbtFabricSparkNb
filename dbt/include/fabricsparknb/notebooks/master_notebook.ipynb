{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[comment]: # (Attach Default Lakehouse Markdown Cell)\n",
    "# üìå Attach Default Lakehouse\n",
    "‚ùó**Note the code in the cell that follows is required to programatically attach the lakehouse and enable the running of spark.sql(). If this cell fails simply restart your session as this cell MUST be the first command executed on session start.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"defaultLakehouse\": {  \n",
    "        \"name\": \"{{lakehouse_name}}\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì¶ Pip\n",
    "Pip installs reqired specifically for this template should occur here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "jsonpickle_loader = importlib.find_loader('jsonpickle')\n",
    "if jsonpickle_loader is None:\n",
    "    print(\"Install jsonpickle\")\n",
    "    !pip install jsonpickle\n",
    "else:\n",
    "    print(\"jsonpickle Already Installed\")\n",
    "\n",
    "tabulate_loader = importlib.find_loader('tabulate')\n",
    "if tabulate_loader is None:\n",
    "    print(\"Install tabulate\")\n",
    "    !pip install tabulate\n",
    "else:\n",
    "    print(\"tabulate Already Installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîó Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils # type: ignore\n",
    "from dataclasses import dataclass\n",
    "import jsonpickle # type: ignore\n",
    "import pandas as pd # type: ignore\n",
    "from tabulate import tabulate # type: ignore\n",
    "import json\n",
    "from pyspark.sql.functions import * # type: ignore\n",
    "import os\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåê Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gv_lakehouse = '{{lakehouse_name}}'\n",
    "gv_log_lakehouse = '{{log_lakehouse}}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #Ô∏è‚É£ Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class NotebookResult:    \n",
    "    notebook: str\n",
    "    start_time: float\n",
    "    status: str\n",
    "    error: str\n",
    "    execution_time: float\n",
    "    run_order: int\n",
    "    \n",
    "@dataclass\n",
    "class FileListing:\n",
    "    \"\"\"Class for Files - Attributes: name, directory\"\"\"\n",
    "    name: str\n",
    "    directory: str\n",
    "\n",
    "def get_file_content_using_notebookutils(file):\n",
    "    \"\"\"Get the content of a file using notebookutils.\"\"\"\n",
    "    #return self.mssparkutils.fs.head(file, 1000000000)\n",
    "    data = spark.sparkContext.wholeTextFiles(file).collect() # type: ignore\n",
    "\n",
    "    # data is a list of tuples, where the first element is the file path and the second element is the content of the file\n",
    "    file_content = data[0][1]\n",
    "\n",
    "    return file_content\n",
    "\n",
    "def remove_file_using_notebookutils(file):\n",
    "    \"\"\"Remove a file using notebookutils.\"\"\"\n",
    "    try:\n",
    "        mssparkutils.fs.rm(file, True)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "def create_path_using_notebookutils(path):\n",
    "    \"\"\"Create a path using notebookutils.\"\"\"\n",
    "    mssparkutils.fs.mkdirs(path)\n",
    "\n",
    "def walk_directory_using_notebookutils(path):\n",
    "    \"\"\"Walk a directory using notebookutils.\"\"\"\n",
    "    # List the files in the directory\n",
    "    files = mssparkutils.fs.ls(path)\n",
    "\n",
    "    # Initialize the list of all files\n",
    "    all_files = []\n",
    "\n",
    "    # Iterate over the files\n",
    "    for file in files:\n",
    "        # If the file is a directory, recursively walk the directory\n",
    "        if file.isDir:\n",
    "            all_files.extend(\n",
    "                walk_directory_using_notebookutils(file.path))\n",
    "        else:\n",
    "            # If the file is not a directory, add it to the list of all files\n",
    "            directory = os.path.dirname(file.path)\n",
    "            name = file.name\n",
    "            all_files.append(FileListing(\n",
    "                name=name, directory=directory))\n",
    "\n",
    "    return all_files\n",
    "\n",
    "def call_child_notebook(notebook, batch_id, master_notebook):\n",
    "        mssparkutils.notebook.run(notebook, {{ notebook_timeout }},{\"pm_batch_id\": batch_id, \"pm_master_notebook\": master_notebook}) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîí Embed HASH information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First make sure that current hash info is the latest for the environment\n",
    "mssparkutils.notebook.run(\"metadata_{{ project_name }}_extract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_hashes = {{ hashes }} # type: ignore\n",
    "RelativePathForMetaData = \"Files/MetaExtracts/\"\n",
    "current_hashes = json.loads(get_file_content_using_notebookutils(RelativePathForMetaData + 'MetaHashes.json'))\n",
    "\n",
    "def get_hash(file, hashes):\n",
    "    ret = \"\"\n",
    "    for h in hashes:\n",
    "        if(h['file'] == file):\n",
    "            return h['hash']\n",
    "    return ret\n",
    "\n",
    "embedded_hashcheck = {{ notebook_hashcheck }} # type: ignore\n",
    "\n",
    "##Hashcheck: BYPASS = 0, WARNING = 1, ERROR = 2\n",
    "if embedded_hashcheck == 0:\n",
    "    print('Metadata Hash Check Bypassed')\n",
    "else:\n",
    "    if current_hashes != embedded_hashes:\n",
    "        for h in embedded_hashes:\n",
    "            print(\n",
    "                    h['file'] + '\\n \\t Emb Hash: ' + get_hash(h['file'], embedded_hashes) + '\\n \\t Env Hash: ' + get_hash(h['file'], current_hashes)\n",
    "            )\n",
    "        if embedded_hashcheck==1:\n",
    "            print('Warning!: Hashes do not match. Its recommended to re-generate the dbt project using the latest extract of the target environment metadata.')\n",
    "        else:\n",
    "            raise Exception('ERROR, Hashes do not match. Its recommended to re-generate the dbt project using the latest extract of the target environment metadata.')\n",
    "    else:\n",
    "        print('Metadata Hashes Match üòè')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÑÔ∏è Prepare Logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create or Alter Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "CREATE TABLE IF NOT EXISTS {gv_log_lakehouse}.execution_log (\n",
    "  notebook STRING,\n",
    "  start_time DOUBLE,\n",
    "  status STRING,\n",
    "  error STRING,\n",
    "  execution_time DOUBLE,\n",
    "  run_order INT,\n",
    "  batch_id string,\n",
    "  master_notebook STRING  \n",
    ")\n",
    "USING DELTA\n",
    "'''\n",
    "\n",
    "spark.sql(sql) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = f'''\n",
    "CREATE TABLE IF NOT EXISTS {gv_log_lakehouse}.batch (\n",
    "  batch_id STRING,\n",
    "  start_time LONG,\n",
    "  status STRING,\n",
    "  master_notebook STRING\n",
    ")\n",
    "USING DELTA\n",
    "'''\n",
    "\n",
    "spark.sql(sql) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the master_notebook column exists in the batch table\n",
    "schema_check_sql = f\"DESCRIBE {gv_log_lakehouse}.execution_log\"\n",
    "schema_check_df = spark.sql(schema_check_sql) # type: ignore\n",
    "\n",
    "# Check if the master_notebook column exists in the schema\n",
    "if 'master_notebook' not in [row['col_name'] for row in schema_check_df.collect()]:\n",
    "    # Add the master_notebook column to the table\n",
    "    alter_table_sql = f'''\n",
    "    ALTER TABLE {gv_log_lakehouse}.execution_log\n",
    "    ADD COLUMN master_notebook STRING\n",
    "    '''\n",
    "    spark.sql(alter_table_sql) # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the master_notebook column exists in the batch table\n",
    "schema_check_sql = f\"DESCRIBE {gv_log_lakehouse}.batch\"\n",
    "schema_check_df = spark.sql(schema_check_sql) # type: ignore\n",
    "\n",
    "# Check if the master_notebook column exists in the schema\n",
    "if 'master_notebook' not in [row['col_name'] for row in schema_check_df.collect()]:\n",
    "    # Add the master_notebook column to the table\n",
    "    alter_table_sql = f'''\n",
    "    ALTER TABLE {gv_log_lakehouse}.batch\n",
    "    ADD COLUMN master_notebook STRING\n",
    "    '''\n",
    "    spark.sql(alter_table_sql) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Related SQL Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def close_batch(batch_id, master_notebook, status):\n",
    "    sql = f'''\n",
    "    UPDATE {gv_log_lakehouse}.batch\n",
    "    SET status = '{status}'\n",
    "    WHERE batch_id = '{str(batch_id)}' \n",
    "    AND master_notebook = '{str(master_notebook)}' '''\n",
    "\n",
    "    spark.sql(sql) # type: ignore\n",
    "\n",
    "def get_open_batch(master_notebook):\n",
    "    sql = f'''\n",
    "    SELECT MAX(batch_id) AS LatestBatchID FROM {gv_log_lakehouse}.batch WHERE status = 'open' AND master_notebook = '{str(master_notebook)}'\n",
    "    '''\n",
    "\n",
    "    return spark.sql(sql).collect()[0]['LatestBatchID'] # type: ignore\n",
    "\n",
    "def insert_new_batch(batch_id, master_notebook):\n",
    "    sql = f'''\n",
    "    INSERT INTO {gv_log_lakehouse}.batch\n",
    "    SELECT '{batch_id}' AS batch_id, UNIX_TIMESTAMP() AS start_time, 'open' AS status, '{str(master_notebook)}' AS master_notebook\n",
    "    '''\n",
    "\n",
    "    spark.sql(sql) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert a New Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_batch_id = str(uuid.uuid4())\n",
    "master_notebook = mssparkutils.runtime.context.get('currentNotebookName')\n",
    "insert_new_batch(new_batch_id, master_notebook) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executions for Each Run Order Below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìú Execution Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the log for this batch execution\n",
    "df_execution_log = spark.sql(f\"SELECT * FROM {gv_log_lakehouse}.execution_log WHERE batch_id = '{new_batch_id}' AND master_notebook = '{master_notebook}'\") # type: ignore\n",
    "# Check if any have not succeeded\n",
    "failed_results = df_execution_log.filter(col(\"status\") != \"success\") # type: ignore\n",
    "succeeded_results = df_execution_log.filter(col(\"status\") == \"success\") # type: ignore\n",
    "\n",
    "if failed_results.count() == 0:    \n",
    "    print(\"Batch Succeeded\")\n",
    "    display(succeeded_results)\n",
    "else:\n",
    "    print(\"Batch Failed\")\n",
    "    display(failed_results)\n",
    "\n",
    "close_batch(new_batch_id, master_notebook, 'closed') # type: ignore\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
