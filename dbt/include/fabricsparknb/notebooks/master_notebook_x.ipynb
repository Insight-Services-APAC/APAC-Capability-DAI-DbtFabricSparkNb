{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #️⃣ Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "pm_batch_id = None\n",
    "pm_log_lakehouse = None\n",
    "pm_master_notebook = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📦 Pip\n",
    "Pip installs reqired specifically for this template should occur here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "jsonpickle_loader = importlib.find_loader('jsonpickle')\n",
    "if jsonpickle_loader is None:\n",
    "    print(\"Install jsonpickle\")\n",
    "    !pip install jsonpickle\n",
    "else:\n",
    "    print(\"jsonpickle Already Installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔗 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils # type: ignore\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "import time\n",
    "import jsonpickle # type: ignore\n",
    "import json\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #️⃣ Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_files1 = {{ notebook_files }}\n",
    "run_order1 = {{ run_order }}\n",
    "\n",
    "# Define a function to execute a notebook and return the results\n",
    "@dataclass\n",
    "class NotebookResult:    \n",
    "    notebook: str\n",
    "    start_time: datetime\n",
    "    status: str\n",
    "    error: str\n",
    "    execution_time: int\n",
    "    run_order: int\n",
    "\n",
    "def execute_notebook(notebook_file):\n",
    "    start_time = datetime.now()\n",
    "\n",
    "    try:\n",
    "        mssparkutils.notebook.run(notebook_file)\n",
    "        status = 'success'\n",
    "        error = None\n",
    "    except Exception as e:\n",
    "        status = 'error'\n",
    "        error = str(e)\n",
    "\n",
    "    end_time = datetime.now()\n",
    "    execution_time_seconds = (end_time - start_time).total_seconds()  # Calculate execution time in seconds\n",
    "    execution_time = int(execution_time_seconds)  # Convert to integer for whole number representation\n",
    "    run_order = run_order1\n",
    "\n",
    "    result = NotebookResult(notebook_file, start_time, status, error, execution_time,run_order)\n",
    "    return result\n",
    "\n",
    "@dataclass\n",
    "class FileListing:\n",
    "    \"\"\"Class for Files - Attributes: name, directory\"\"\"\n",
    "    name: str\n",
    "    directory: str\n",
    "\n",
    "def get_file_content_using_notebookutils(file):\n",
    "    \"\"\"Get the content of a file using notebookutils.\"\"\"\n",
    "    #return self.mssparkutils.fs.head(file, 1000000000)\n",
    "    data = spark.sparkContext.wholeTextFiles(file).collect() # type: ignore\n",
    "\n",
    "    # data is a list of tuples, where the first element is the file path and the second element is the content of the file\n",
    "    file_content = data[0][1]\n",
    "\n",
    "    return file_content\n",
    "\n",
    "def create_path_using_notebookutils(path):\n",
    "    \"\"\"Create a path using notebookutils.\"\"\"\n",
    "    mssparkutils.fs.mkdirs(path)\n",
    "\n",
    "def walk_directory_using_notebookutils(path):\n",
    "    \"\"\"Walk a directory using notebookutils.\"\"\"\n",
    "    # List the files in the directory\n",
    "    files = mssparkutils.fs.ls(path)\n",
    "\n",
    "    # Initialize the list of all files\n",
    "    all_files = []\n",
    "\n",
    "    # Iterate over the files\n",
    "    for file in files:\n",
    "        # If the file is a directory, recursively walk the directory\n",
    "        if file.isDir:\n",
    "            all_files.extend(\n",
    "                walk_directory_using_notebookutils(file.path))\n",
    "        else:\n",
    "            # If the file is not a directory, add it to the list of all files\n",
    "            directory = os.path.dirname(file.path)\n",
    "            name = file.name\n",
    "            all_files.append(FileListing(\n",
    "                name=name, directory=directory))\n",
    "\n",
    "    return all_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Execution Log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"notebook\", StringType(), True),\n",
    "    StructField(\"start_time\", TimestampType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"error\", StringType(), True),\n",
    "    StructField(\"execution_time\", IntegerType(), True),\n",
    "    StructField(\"run_order\", IntegerType(), True),\n",
    "    StructField(\"batch_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create an empty DataFrame with the defined schema\n",
    "failed_results = spark.createDataFrame([], schema=schema)\n",
    "# Read the log for this batch execution\n",
    "df_execution_log = spark.sql(f\"SELECT * FROM {pm_log_lakehouse}.execution_log WHERE batch_id = '{pm_batch_id}' AND master_notebook = '{pm_master_notebook}'\")\n",
    "if df_execution_log.count() > 0:\n",
    "    \n",
    "    # Check if any have not succeeded\n",
    "    failed_results = df_execution_log.filter(col(\"status\") != \"success\")\n",
    "\n",
    "    # Print the failed results\n",
    "    for row in failed_results.collect():\n",
    "        print(f\"Notebook {row['notebook']} failed with error: {row['error']}\")\n",
    "\n",
    "    # Check if have succeeded\n",
    "    succeeded_results = df_execution_log.filter(col(\"status\") == \"success\")\n",
    "\n",
    "    # Print the succeeded results\n",
    "    for row in succeeded_results.collect():\n",
    "        print(f\"Notebook {row['notebook']} succeeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Notebooks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the Log DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"notebook\", StringType(), True),\n",
    "    StructField(\"start_time\", TimestampType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"error\", StringType(), True),\n",
    "    StructField(\"execution_time\", IntegerType(), True),\n",
    "    StructField(\"run_order\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "if failed_results.count() == 0:\n",
    "    new_results = []\n",
    "    # Use a ThreadPoolExecutor to run the notebooks in parallel\n",
    "    # Execute the notebooks and collect the results\n",
    "    with ThreadPoolExecutor(max_workers={{ max_worker }}) as executor:\n",
    "        new_results = list(executor.map(execute_notebook, notebook_files1))\n",
    "\n",
    "    # Write the results to the log file\n",
    "    df_log = spark.createDataFrame(new_results, schema=schema)\n",
    "    df_log = df_log.withColumn(\"batch_id\", lit(f'{pm_batch_id}'))\n",
    "    df_log = df_log.withColumn(\"master_notebook\", lit(f'{pm_master_notebook}'))\n",
    "    df_log.write.format(\"delta\").mode(\"append\").saveAsTable(\"{pm_log_lakehouse}.execution_log\")\n",
    "else:\n",
    "    print(\"Failures in previous run_order... supressing execution\")\n",
    "    raise Exception(\"Failures in previous run_order... supressing execution\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
