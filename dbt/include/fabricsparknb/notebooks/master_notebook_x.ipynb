{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Œ Attach Default Lakehouse\n",
    "â—**Note the code in the cell that follows is required to programatically attach the lakehouse and enable the running of spark.sql(). If this cell fails simply restart your session as this cell MUST be the first command executed on session start.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%configure\n",
    "{\n",
    "    \"defaultLakehouse\": {  \n",
    "        \"name\": \"{{lakehouse_name}}\",\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“¦ Pip\n",
    "Pip installs reqired specifically for this template should occur here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jsonpickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ”— Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebookutils import mssparkutils # type: ignore\n",
    "from dataclasses import dataclass\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import json\n",
    "import time\n",
    "import jsonpickle # type: ignore\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #ï¸âƒ£ Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_files1 = {{ notebook_files }}\n",
    "run_order1 = {{ run_order }}\n",
    "\n",
    "# Define a function to execute a notebook and return the results\n",
    "@dataclass\n",
    "class NotebookResult:    \n",
    "    notebook: str\n",
    "    start_time: float\n",
    "    status: str\n",
    "    error: str\n",
    "    execution_time: float\n",
    "    run_order: int\n",
    "\n",
    "def execute_notebook(notebook_file):\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        mssparkutils.notebook.run(notebook_file)\n",
    "        status = 'success'\n",
    "        error = None\n",
    "    except Exception as e:\n",
    "        status = 'error'\n",
    "        error = str(e)\n",
    "\n",
    "    execution_time = time.time() - start_time\n",
    "    run_order = run_order1\n",
    "\n",
    "    result = NotebookResult(notebook_file, start_time, status, error, execution_time,run_order)\n",
    "    return result\n",
    "\n",
    "@dataclass\n",
    "class FileListing:\n",
    "    \"\"\"Class for Files - Attributes: name, directory\"\"\"\n",
    "    name: str\n",
    "    directory: str\n",
    "\n",
    "def get_file_content_using_notebookutils(file):\n",
    "    \"\"\"Get the content of a file using notebookutils.\"\"\"\n",
    "    #return self.mssparkutils.fs.head(file, 1000000000)\n",
    "    data = spark.sparkContext.wholeTextFiles(file).collect() # type: ignore\n",
    "\n",
    "    # data is a list of tuples, where the first element is the file path and the second element is the content of the file\n",
    "    file_content = data[0][1]\n",
    "\n",
    "    return file_content\n",
    "\n",
    "def create_path_using_notebookutils(path):\n",
    "    \"\"\"Create a path using notebookutils.\"\"\"\n",
    "    mssparkutils.fs.mkdirs(path)\n",
    "\n",
    "def walk_directory_using_notebookutils(path):\n",
    "    \"\"\"Walk a directory using notebookutils.\"\"\"\n",
    "    # List the files in the directory\n",
    "    files = mssparkutils.fs.ls(path)\n",
    "\n",
    "    # Initialize the list of all files\n",
    "    all_files = []\n",
    "\n",
    "    # Iterate over the files\n",
    "    for file in files:\n",
    "        # If the file is a directory, recursively walk the directory\n",
    "        if file.isDir:\n",
    "            all_files.extend(\n",
    "                walk_directory_using_notebookutils(file.path))\n",
    "        else:\n",
    "            # If the file is not a directory, add it to the list of all files\n",
    "            directory = os.path.dirname(file.path)\n",
    "            name = file.name\n",
    "            all_files.append(FileListing(\n",
    "                name=name, directory=directory))\n",
    "\n",
    "    return all_files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Execution Log\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"notebook\", StringType(), True),\n",
    "    StructField(\"start_time\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"error\", StringType(), True),\n",
    "    StructField(\"execution_time\", DoubleType(), True),\n",
    "    StructField(\"run_order\", IntegerType(), True),\n",
    "    StructField(\"batch_id\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Create an empty DataFrame with the defined schema\n",
    "failed_results = spark.createDataFrame([], schema=schema)\n",
    "#Get latest open batch\n",
    "latest_batch_id = spark.sql(\"SELECT MAX(batch_id) AS LatestBatchID FROM batch WHERE status = 'open'\").collect()[0]['LatestBatchID']\n",
    "# Read the log for this batch execution\n",
    "df_execution_log = spark.sql(f\"SELECT * FROM execution_log WHERE batch_id = {latest_batch_id}\")\n",
    "if df_execution_log.count() > 0:\n",
    "    \n",
    "    # Check if any have not succeeded\n",
    "    failed_results = df_execution_log.filter(col(\"status\") != \"success\")\n",
    "\n",
    "    # Print the failed results\n",
    "    for row in failed_results.collect():\n",
    "        print(f\"Notebook {row['notebook']} failed with error: {row['error']}\")\n",
    "\n",
    "    # Check if have succeeded\n",
    "    succeeded_results = df_execution_log.filter(col(\"status\") == \"success\")\n",
    "\n",
    "    # Print the succeeded results\n",
    "    for row in succeeded_results.collect():\n",
    "        print(f\"Notebook {row['notebook']} succeeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute Notebooks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the schema for the Log DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"notebook\", StringType(), True),\n",
    "    StructField(\"start_time\", DoubleType(), True),\n",
    "    StructField(\"status\", StringType(), True),\n",
    "    StructField(\"error\", StringType(), True),\n",
    "    StructField(\"execution_time\", DoubleType(), True),\n",
    "    StructField(\"run_order\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "if failed_results.count() == 0:\n",
    "    new_results = []\n",
    "    # Use a ThreadPoolExecutor to run the notebooks in parallel\n",
    "    # Execute the notebooks and collect the results\n",
    "    with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "        new_results = list(executor.map(execute_notebook, notebook_files1))\n",
    "\n",
    "    # Write the results to the log file\n",
    "    df_log = spark.createDataFrame(new_results, schema=schema)\n",
    "    df_log = df_log.withColumn(\"batch_id\", lit(latest_batch_id))\n",
    "    df_log.write.format(\"delta\").mode(\"append\").saveAsTable(\"execution_log\")\n",
    "else:\n",
    "    print(\"Failures in previous run_order... supressing execution\")\n",
    "    raise Exception(\"Failures in previous run_order... supressing execution\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
